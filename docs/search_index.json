[["index.html", "Introduction Software Basics Author Team", " Opioid Environment Toolkit Developed for the JCOIN Network of the NIH HEAL Initiative. Last Updated : 2020-10-08 Introduction This toolkit provides an introduction to GIS and spatial analysis for opioid environment applications that will allow researchers, analysts, and practitioners to support their communities with better data analytics and visualization services. We introduce basic spatial analytic functionalities using open source tools, mainly in R, using applied examples for visualizing, mapping, and understanding the opioid risk environment. Software Basics We assume a basic knowledge of R and coding languages for these toolkits. For most of the tutorials in this toolkit, you’ll need to have R and RStudio downloaded and installed on your system. You should be able to install packages, know how to find the address to a folder on your computer system, and have very basic familiarity with R. If you are new to R, we recommend the following intro-level tutorials provided through installation guides. You can also refer to this R for Social Scientists tutorial developed by Data Carpentry for a refresher. We will work with following libraries, so please be sure to install: sf tmap tidyverse tidycensus There are differing spatial ecosystems in R. We use the sf ecosystem that is compatible with the tidyverse. If you need to work between these two R spatial ecosystems, see this guide for a translation of sp to sf commands. Author Team This toolkit was developed for the JCOIN network by Marynia Kolak, Moksha Menghaney, Qinyun Lin, and Angela Li at the Center for Spatial Data Science at the University of Chicago as part of the Methodology and Advanced Analytics Resource Center (MAARC). JCOIN is part of the of the NIH HEAL Initiative. The Helping to End Addiction Long-term InitiativeSM, or NIH HEAL InitiativeSM, supports a wide range of programs to develop new or improved prevention and treatment strategies for opioid addiction. "],["spatial-data-introduction.html", "1 Spatial Data Introduction 1.1 Defining spatial data 1.2 Spatial data formats 1.3 Spatial data types 1.4 Coordinate Reference System Further resources", " 1 Spatial Data Introduction 1.1 Defining spatial data Spatial data refers to data that contain information about specific locations, and the information content of the data may change with location. In other words, “information” and “location” are two important elements in spatial data. On some occasions, spatial data may only include “location.” But without “location,” the data is no longer spatial anymore. For example, a spatial data that describes the resource distribution of Medications for Opioid Overuse Disorder (MOUDs) must contain and enable location information of these MOUD resources, otherwise the data becomes a non-spatial list of those resources. For the purpose of this tutorial, we will only briefly introduce some important concepts in spatial data. See Further Resources if you would like to learn more about these concepts. 1.2 Spatial data formats Spatial data can be stored in a text file like comma-separated value (CSV) files. However, the text file needs to have essential location columns with latitude and longitude to represent the coordinate location of a spatial object. Furthermore, a CSV file with lat/long columns is only a flat/non-spatial file, until the spatial location context is enabled as a new spatial data format. A common spatial data format is the shapefile, which comes from ESRI/ArcGIS proprietary software. The shapefile file format (.shp for short) includes a minimum of 4 files, with a common prefix and different filename extensions .shp, .shx, .dbf, and .prj. In order to work with the spatial data, we need all these four components of the shapefile stored in the same directory, so that the software (such as R) can know how to project spatial objects onto a geographic or coordinate space (i.e., spatial location context is enabled). Other common spatial data formats include the GeoJSON, KML, and geopackage. 1.2.1 Simple features Simple features refers to an international standard (ISO 19125-1:2004) that describes how real-world objects, and their spatial geometries, are represented in computers. This standard is enabled in ESRI/ArcGIS architecture, POSTGIS (a spatial extension for PostGresSQL), the GDAL libraries that serve as underpinnings to most GIS work, and GeoJSONs. The sf R ecoystem makes simple features even more accessible within R, so that simple feature objects in spatial data are also stored in a data frame, with one vector/column containing geographic data (usually named “geometry” or “geom”). Why should you care about these computational components of spatial systems architecture? Spatial analysis often requires troubleshooting, and spatial formats can be complicated. We recommend using the str function in R to familiarize yourself with data objects as you go, and to explore the data files as well. For example: a shapefile includes (at least) four components, separating the data as a .dbf file and projection information as a .prj file. In contrast, the sf spatial file loaded in your R environment is one unified object, has the spatial information recorded as a geometry field, and projection stored as metadata. 1.3 Spatial data types Two common formats of spatial data are vector and raster data. For the purpose of this tutorial, we will focus on vector data that represents the world surface using points, lines, and polygons. Connecting points can generate lines, and connecting lines that create an enclosed area can generate polygons. Below we use sf to create some commonly used geometry types based on points, lines, and polygons. For our purposes, think of further simplifying these to points, lines, and polygons. A group of clinics can be geocoded and converted to points, whereas zip code boudnaries are represented as polygons. library(sf) # create a XY point point &lt;- st_point(c(2,4)) # create a multipoint that includes five points multipoint &lt;- st_multipoint(rbind(c(2,2), c(2, 4), c(4, 2), c(4, 4), c(3,3))) par(mfrow=c(1, 2)) plot(point, main = &quot;point&quot;) plot(multipoint, main = &quot;multipoint&quot;) # create a linestring by combining three points linestring &lt;- st_linestring(rbind(c(2,1), c(2,2), c(2,3))) # create a multilinestring multilinestring &lt;- st_multilinestring(list(rbind(c(2,1), c(2,2), c(2,3)), rbind(c(2,4), c(4,3)))) par(mfrow=c(1,2)) plot(linestring, main = &quot;linestring&quot;) plot(multilinestring, main = &quot;multistring&quot;) # create a polygon polygon &lt;- st_polygon(list(rbind(c(1,1), c(1, 4), c(4, 4), c(4, 1), c(1,1)))) # create multipolygon multipolygon &lt;- st_multipolygon(list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))), list(rbind(c(0, 2), c(1, 2), c(1, 2), c(0, 4), c(0, 2))))) par(mfrow=c(1,2)) plot(polygon, main = &quot;polygon&quot;) plot(multipolygon, main = &quot;multipolygon&quot;) Read more regarding vector and raster data from Chapter 2 Geographic data in R of the Lovelace et al 2019 text, Geocomputation with R. This opensource text is an incredible resource for those who are interested in learning more details regarding geographic data analysis, visualization, and modeling, and represents one of dozens of resources available for learning and honing R &amp; spatial analysis skills. 1.4 Coordinate Reference System As noted before, the most fundamental element of a spatial data is “location.” A Coordinate Reference System (CRS) communicates what method should be used to flatten or project the Earth’s surface onto a 2-dimensional map. Importantly, different CRS implies different ways of projections and generates substantially different visualizations. For example, following are some world maps using different projections. (Check out here for more different world map projections.) Figure 1.1: The whole wide world by PeterFisken is under CC BY 2.0 Figure 1.2: Peters Projection World Map (35’x51’) by inju is under CC BY-NC-SA 2.0 Figure 1.3: Hammond Cylindrical Projection World Map 1905 by perpetualplum is licensed under CC BY 2.0 Because different CRS imply different ways of projections and generates substantially different visualizations, it is important to make sure the CRS accompanied with each spatial data are the same before implementing any advanced spatial analysis or geometric processing. In sf, you can use the function st_crs to check the CRS used in one data, and the function st_transform to project the data to a particular CRS. See this Interactive Tutorial that demonstrates these functions. The CRS concept can be tricky to comprehend and utilize in practice, so we will come back to this several times in our tutorials. Further resources See Chapter 2 Geographic data in R in Geocomputation with R for more info about Vector data, Raster data, and Coordiante Reference Systems. See this Software Carpentry workshop for more explanations to better understand coordinate reference systems. See this Interactive Tutorial that uses sf package to project spatial data in R. "],["geocodingAddress-tutorial.html", "2 Geocoding Resource Locations 2.1 Overview 2.2 Environment Setup 2.3 Geocode addresses 2.4 Convert to Spatial Data", " 2 Geocoding Resource Locations 2.1 Overview A common goal in opioid environment research is to calculate and compare access metrics to different providers of Medications for Opioid Overuse Disorder (MOUDs). Before we can run any analytics on the resource location data, we need to convert resource addresses to spatial data points, which can be then used to calculate access metrics. Geocoding is the process of converting addresses (like a street address) into geographic coordinates using a known coordinate reference system. We can then use these coordinates (latitude, longitude) to spatially enable our data. This means we convert to a spatial data frame (sf) within R for spatial analysis within our R session, and then save as a shapefile (a spatial data format) for future use. In this tutorial we demonstrate how to geocode resource location addresses and convert to spatial data points that can be used for future mapping and geospatial analysis. Our objectives are thus to: Geocode addresses to get geographic coordinates Visualize the resource locations as points on a map in R Transform a flat file (.CSV) to a spatially enabled shapefile (.SHP) 2.2 Environment Setup To replicate the code &amp; functions illustrated in this tutorial, you’ll need to have R and RStudio downloaded and installed on your system. This tutorial assumes some familiarity with the R programming language. 2.2.1 Input/Output Our input will be a CSV file that include addresses of our resources. This files can be found here. Chicago Methadone Clinics, chicago_methadone.csv We will convert these addresses to geographic coordinates using an appropriate coordinate reference system (CRS), and then spatially enable the data for mapping. We will then export the spatial dataframe as a shapefile. 2.2.2 Load Libraries We will use the following packages in this tutorial: sf: to manipulate spatial data tmap: to visualize and create maps tidygeocoder: to convert addresses to geographic coordinates Then load the libraries for use. Note: The messages you see about GEOS, GDAL, and PROJ refer to software libraries that allow you to work with spatial data. library(sf) library(tidygeocoder) library(tmap) 2.2.3 Load Data We will use a CSV that includes methadone clinic addresses in Chicago as an example. We start with a small dataset to test our geocoding workflow, as best practice. Let’s take a look at the first few rows of the dataset. Our data includes addresses but not geographic coordinates. methadoneClinics &lt;- read.csv(&quot;data/chicago_methadone_nogeometry.csv&quot;) head(methadoneClinics) ## X Name ## 1 1 Chicago Treatment and Counseling Center, Inc. ## 2 2 Sundace Methadone Treatment Center, LLC ## 3 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview ## 4 4 PDSSC - Chicago, Inc. ## 5 5 Center for Addictive Problems, Inc. ## 6 6 Family Guidance Centers, Inc. ## Address City State Zip ## 1 4453 North Broadway st. Chicago IL 60640 ## 2 4545 North Broadway St. Chicago IL 60640 ## 3 3934 N. Lincoln Ave. Chicago IL 60613 ## 4 2260 N. Elston Ave. Chicago IL 60614 ## 5 609 N. Wells St. Chicago IL 60654 ## 6 310 W. Chicago Ave. Chicago IL 60654 2.3 Geocode addresses 2.3.1 Quality Control Before geocoding, perform an initial quality check on the data. Note that the address, city, state, and zip code are all separated as different columns. This will make it easier to stitch together for a coherent, standard address for geocoding. Furthermore, there do not appear to be any major errors. The city name “Chicago” is spelled consistently, without missing addresses or zip codes. This will not always be the case, unfortunately. Data must be cleaned prior to loading into a geocoding service. 2.3.2 Selecting Geocoding Service To get a geographic coordinate for each site, we’ll need to geocode. There are a number of geocoding options in R; here we use we the tidygeocoder package. It uses mutliple geocoding services, providing the user with an option to choose. It also provides the option to use a cascade method which queries other geocoding services incase the default method fails to provide coordinates. When considering which geocoding service to use, consider scale and potential geocoding errors. Some geocoding services are more accurate than others, so if your coordinates were not coded precisely, try a different service. If you have thousands of addresses to geocode, you may require more complex data pipelines. The default method used here is via US Census geocoder, which allows around 10,000 addresses to be geocoded at once. Others have varying daily limits. The Google Maps API and ESRI Geocoding service are additional high-quality geocoding services with varying cost associated. 2.3.3 Test Geocoding Service Before geocoding your entire dataset, first review the documentation for the geocoding service you’ll be using. In our example we use tidygeocoder, with documentation found here. Let’s test the service by starting with one address: sample &lt;- geo(&quot;4545 North Broadway St. Chicago, IL&quot;, lat = latitude, long = longitude, method = &#39;cascade&#39;) sample ## # A tibble: 1 x 4 ## address latitude longitude geo_method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4545 North Broadway St. Chicago, … 42.0 -87.7 census What did the output look like? Get familiar with the input parameters, expected output, and review the documentation further if needed. 2.3.4 Prepare input parameter To apply the function to multiple addresses, we first we need ensure that we have a character vector of full addresses. str(methadoneClinics) ## &#39;data.frame&#39;: 27 obs. of 6 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Name : chr &quot;Chicago Treatment and Counseling Center, Inc.&quot; &quot;Sundace Methadone Treatment Center, LLC&quot; &quot;Soft Landing Interventions/DBA Symetria Recovery of Lakeview&quot; &quot;PDSSC - Chicago, Inc.&quot; ... ## $ Address: chr &quot;4453 North Broadway st.&quot; &quot;4545 North Broadway St.&quot; &quot;3934 N. Lincoln Ave.&quot; &quot;2260 N. Elston Ave.&quot; ... ## $ City : chr &quot;Chicago&quot; &quot;Chicago&quot; &quot;Chicago&quot; &quot;Chicago&quot; ... ## $ State : chr &quot;IL&quot; &quot;IL&quot; &quot;IL&quot; &quot;IL&quot; ... ## $ Zip : int 60640 60640 60613 60614 60654 60654 60651 60607 60607 60616 ... Next we convert all fields to character first to avoid issues with factors (a common peril of R!). methadoneClinics$fullAdd &lt;- paste(as.character(methadoneClinics$Address), as.character(methadoneClinics$City), as.character(methadoneClinics$State), as.character(methadoneClinics$Zip)) head(methadoneClinics) ## X Name ## 1 1 Chicago Treatment and Counseling Center, Inc. ## 2 2 Sundace Methadone Treatment Center, LLC ## 3 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview ## 4 4 PDSSC - Chicago, Inc. ## 5 5 Center for Addictive Problems, Inc. ## 6 6 Family Guidance Centers, Inc. ## Address City State Zip ## 1 4453 North Broadway st. Chicago IL 60640 ## 2 4545 North Broadway St. Chicago IL 60640 ## 3 3934 N. Lincoln Ave. Chicago IL 60613 ## 4 2260 N. Elston Ave. Chicago IL 60614 ## 5 609 N. Wells St. Chicago IL 60654 ## 6 310 W. Chicago Ave. Chicago IL 60654 ## fullAdd ## 1 4453 North Broadway st. Chicago IL 60640 ## 2 4545 North Broadway St. Chicago IL 60640 ## 3 3934 N. Lincoln Ave. Chicago IL 60613 ## 4 2260 N. Elston Ave. Chicago IL 60614 ## 5 609 N. Wells St. Chicago IL 60654 ## 6 310 W. Chicago Ave. Chicago IL 60654 2.3.5 Batch Geocoding Now we are ready to geocode the addresses. The “tibble” data structure below shows us the address, latitude, longitude and also the geocoding service used to get the coordinates. Note that geocoding takes a bit of time, so patience is required. geoCodedClinics &lt;- methadoneClinics %&gt;% geocode(methadoneClinics, address = &#39;fullAdd&#39;, lat = latitude, long = longitude, method = &#39;cascade&#39;) geoCodedClinics ## # A tibble: 27 x 10 ## X Name Address City State Zip fullAdd latitude ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 &quot;Chi… 4453 N… Chic… IL 60640 4453 N… NA ## 2 2 &quot;Sun… 4545 N… Chic… IL 60640 4545 N… NA ## 3 3 &quot;Sof… 3934 N… Chic… IL 60613 3934 N… 42.0 ## 4 4 &quot;PDS… 2260 N… Chic… IL 60614 2260 N… 41.9 ## 5 5 &quot;Cen… 609 N.… Chic… IL 60654 609 N.… 41.9 ## 6 6 &quot;Fam… 310 W.… Chic… IL 60654 310 W.… 41.9 ## 7 7 &quot;A R… 3809 W… Chic… IL 60651 3809 W… 41.9 ## 8 8 &quot;*&quot; 140 N.… Chic… IL 60607 140 N.… 41.9 ## 9 9 &quot;Hea… 210 N.… Chic… IL 60607 210 N.… 41.9 ## 10 10 &quot;Spe… 2630 S… Chic… IL 60616 2630 S… 41.8 ## # … with 17 more rows, and 2 more variables: longitude &lt;dbl&gt;, ## # geo_method &lt;chr&gt; The code worked for all addresses except the first two. We already resolved the 4545 North Broadway St.address above but here in the dataframe we get NAs. It is pointing to some issue with the string input. These were missed in the previous quality check, but give us a clue to the types of errors we could see if geocoding more addresses. Unfortunately, such quirks are common across geocoding services in R and we just have to handle them. We manually update the full address strings to get apprpriate coordinates. methadoneClinics[1,&#39;fullAdd&#39;] &lt;- &#39;4453 North Broadway St.,Chicago IL 60640&#39; methadoneClinics[2,&#39;fullAdd&#39;] &lt;- &#39;4545 North Broadway St.,Chicago IL 60640&#39; Now we can geocode the full suite of addresses with success: geoCodedClinics &lt;- methadoneClinics %&gt;% geocode(methadoneClinics, address = &#39;fullAdd&#39;, lat = latitude, long = longitude, method = &#39;cascade&#39;) geoCodedClinics ## # A tibble: 27 x 10 ## X Name Address City State Zip fullAdd latitude ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 &quot;Chi… 4453 N… Chic… IL 60640 4453 N… 42.0 ## 2 2 &quot;Sun… 4545 N… Chic… IL 60640 4545 N… 42.0 ## 3 3 &quot;Sof… 3934 N… Chic… IL 60613 3934 N… 42.0 ## 4 4 &quot;PDS… 2260 N… Chic… IL 60614 2260 N… 41.9 ## 5 5 &quot;Cen… 609 N.… Chic… IL 60654 609 N.… 41.9 ## 6 6 &quot;Fam… 310 W.… Chic… IL 60654 310 W.… 41.9 ## 7 7 &quot;A R… 3809 W… Chic… IL 60651 3809 W… 41.9 ## 8 8 &quot;*&quot; 140 N.… Chic… IL 60607 140 N.… 41.9 ## 9 9 &quot;Hea… 210 N.… Chic… IL 60607 210 N.… 41.9 ## 10 10 &quot;Spe… 2630 S… Chic… IL 60616 2630 S… 41.8 ## # … with 17 more rows, and 2 more variables: longitude &lt;dbl&gt;, ## # geo_method &lt;chr&gt; 2.4 Convert to Spatial Data While we have geographic coordinates loaded in our data, it is still not spatially enabled. To convert to a spatial data format, we have to enable to coordinate reference system that connects the latitude and longitude recorded to actual points on Earth. 2.4.1 Spatial Reference Systems There are thousands of ways to model the Earth, and each requires a different spatial reference system. This is a very complicated domain of spatial applications (for a primer see here), but for our purposes, we simplify by using a geodetic CRS that uses coordinates longitude and latitude. Not all coordinates will appear as a latitude/longitude, however, so it’s important to at least check for the CRS used when working with geographic data. The lat/long coordinates provided by the geocoding service we used report data using the CRS coded as 4326, a World Geodetic System (WGS84) model also used by Google Earth and many other applications. In this system, distance is measured as degrees and distorted. So while useful for visualizing points, we will need to convert to another CRS for other types of spatial analysis. 2.4.2 Enable Points Next we convert our dataframe to a spatial data frame using the st_as_sf() function. The coords argument specifies which two columns are the X and Y for your data. We set the crs argument equal to 4326. Please note longitude is entered as first column rather than the latitude. It is a very common mistake. The X, Y field actually refers to longitude, latitude. methadoneSf &lt;- st_as_sf(geoCodedClinics, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) head(data.frame(methadoneSf)) ## X Name ## 1 1 Chicago Treatment and Counseling Center, Inc. ## 2 2 Sundace Methadone Treatment Center, LLC ## 3 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview ## 4 4 PDSSC - Chicago, Inc. ## 5 5 Center for Addictive Problems, Inc. ## 6 6 Family Guidance Centers, Inc. ## Address City State Zip ## 1 4453 North Broadway st. Chicago IL 60640 ## 2 4545 North Broadway St. Chicago IL 60640 ## 3 3934 N. Lincoln Ave. Chicago IL 60613 ## 4 2260 N. Elston Ave. Chicago IL 60614 ## 5 609 N. Wells St. Chicago IL 60654 ## 6 310 W. Chicago Ave. Chicago IL 60654 ## fullAdd geo_method ## 1 4453 North Broadway St.,Chicago IL 60640 osm ## 2 4545 North Broadway St.,Chicago IL 60640 osm ## 3 3934 N. Lincoln Ave. Chicago IL 60613 census ## 4 2260 N. Elston Ave. Chicago IL 60614 census ## 5 609 N. Wells St. Chicago IL 60654 census ## 6 310 W. Chicago Ave. Chicago IL 60654 census ## geometry ## 1 POINT (-87.65566 41.96321) ## 2 POINT (-87.65694 41.96475) ## 3 POINT (-87.67818 41.95331) ## 4 POINT (-87.67407 41.92269) ## 5 POINT (-87.63409 41.89268) ## 6 POINT (-87.63636 41.89657) Note that this is a data frame, but that it has a final column called “geometry” that stores the spatial information. 2.4.3 Visualize Points We can now plot the location of the methadone clinics with base R. This is a recommended step to confirm that you translated your coordinates correctly. A common mistake is switching the lat/long values, so your points could plot across the globe. If that happens, repeat the step above with flipped long/lat values. First we switch the tmap mode to view so we can look at the points with a live basemap layer. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing Next, we plot our points as dots, and add the basemap. tm_shape(methadoneSf) + tm_dots() + tm_basemap(&quot;OpenStreetMap&quot;) 2.4.4 Convert to Shapefile Finally, we save this spatial dataframe as a shapefile which can be used for further spatial analysis. write_sf(methadoneSf, &quot;data/methadoneClinics.shp&quot;) "],["buffer_analysis.html", "3 Buffer Analysis 3.1 Overview 3.2 Environment Setup 3.3 Simple Overlay Map 3.4 Spatial Transformation 3.5 Generate Buffers 3.6 Rinse &amp; Repeat", " 3 Buffer Analysis 3.1 Overview Once we have spatially referenced resource locations, it’s helpful to plot the data in the community of interest for some preliminary analysis. In this tutorial we will plot Methadone Providers in Chicago and community areas to provide some context. We will also generate a simple 1-mile buffer service area around each provider to highlight neighborhoods with better, and worse, access to resources. In order to accomplish this task, we will need to standardize our spatial data (clinic points, and community areas) with an appropriate coordinate reference system. Finally, we’ll make some maps! Our objectives are thus to: Overlay clinical providers (points) and community areas (polygons) Use a spatial transform operation to change coordinate reference systems Conduct a simple buffer analysis 3.2 Environment Setup To replicate the code &amp; functions illustrated in this tutorial, you’ll need to have R and RStudio downloaded and installed on your system. This tutorial assumes some familiarity with the R programming language. 3.2.1 Input/Output Our inputs will be two shapefiles, and a geojson (all spatial file formats). These files can be found here, though the providers point file was generated in the Geocoding tutorial. Note that all four files are required (.dbf, .prj, .shp, and .shx) to consitute a shapefile. Chicago Methadone Clinics, methadoneClinics.shp Chicago Zip Codes, chicago_zips.shp Chicago City Boundary, BoundariesChicago.geojson We will generate a 1-mile buffer around each point, and generate maps with the zip code areas for context. We will also export the final buffer areas as another shapefile for future use. Finally, we’ll generate a more beautiful map by including the city boundary. If you don’t have a shapefile of your data, but already have geographic coordinates as two columns in your CSV file, you can still use this tutorial. A reminder of how to transform your CSV with coordinates into a spatial data frame in R can be found here. 3.2.2 Load Libraries We will use the following packages in this tutorial: sf: to manipulate spatial data tmap: to visualize and create maps First, load the required libraries. library(sf) library(tmap) 3.2.3 Load Data Load in the MOUD resources shapefile. MetClinics &lt;- st_read(&quot;data/methadoneClinics.shp&quot;) ## Reading layer `methadoneClinics&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/data/methadoneClinics.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 27 features and 8 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -87.7349 ymin: 41.68698 xmax: -87.57673 ymax: 41.96475 ## geographic CRS: WGS 84 Next, we load a shapefile of Chicago zip codes. You can often find shapefiles (or spatial data formats like geojson) on city data portals for direct download. We will walk you through downloading zip code boundaries directly through the Census via R in a later tutorial. Areas &lt;- st_read(&quot;data/chicago_zips.shp&quot;) ## Reading layer `chicago_zips&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/data/chicago_zips.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 85 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.06058 ymin: 41.58152 xmax: -87.52366 ymax: 42.06504 ## geographic CRS: WGS 84 CityBoundary &lt;- st_read(&quot;data/BoundariesChicago.geojson&quot;) ## Reading layer `BoundariesChicago&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/data/BoundariesChicago.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 1 feature and 4 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -87.94011 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304 ## geographic CRS: WGS 84 Quickly view the first few rows of the zip codes and clinics using your favorite function (head, glimpse, str, and so forth). head(Areas) ## Simple feature collection with 6 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.06058 ymin: 41.73452 xmax: -87.58209 ymax: 42.04052 ## geographic CRS: WGS 84 ## ZCTA5CE10 GEOID10 CLASSFP10 MTFCC10 FUNCSTAT10 ALAND10 ## 1 60501 60501 B5 G6350 S 12532295 ## 2 60007 60007 B5 G6350 S 36493383 ## 3 60651 60651 B5 G6350 S 9052862 ## 4 60652 60652 B5 G6350 S 12987857 ## 5 60653 60653 B5 G6350 S 6041418 ## 6 60654 60654 B5 G6350 S 1464813 ## AWATER10 INTPTLAT10 INTPTLON10 ## 1 974360 +41.7802209 -087.8232440 ## 2 917560 +42.0086000 -087.9973398 ## 3 0 +41.9020934 -087.7408565 ## 4 0 +41.7479319 -087.7147951 ## 5 1696670 +41.8199645 -087.6059654 ## 6 113471 +41.8918225 -087.6383036 ## geometry ## 1 MULTIPOLYGON (((-87.86289 4... ## 2 MULTIPOLYGON (((-88.06058 4... ## 3 MULTIPOLYGON (((-87.77559 4... ## 4 MULTIPOLYGON (((-87.74205 4... ## 5 MULTIPOLYGON (((-87.62623 4... ## 6 MULTIPOLYGON (((-87.64775 4... 3.3 Simple Overlay Map We can plot these quickly using the tmap library to ensure they are overlaying correctly. If they are, our coordinate systems are working correctly. When using tmap the first parameter references the spatial file we’d like to map (tm_shape), and the next parameter(s) indicate how we want to style the data. For polygons, we can style tm_borders to have a slightly transparent boundary. For the point data, we will use red dots that are sized appropriately using the tm_dots parameter. When working with tmap or any other library for the first time, it’s helpful to review the documentation and related tutorials for more tips on usability. We use the tmap “plot” view mode to view the data in a static format. tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting ## 1st layer (gets plotted first) tm_shape(Areas) + tm_borders(alpha = 0.4) + ## 2nd layer (overlay) tm_shape(MetClinics) + tm_dots(size = 0.4, col=&quot;red&quot;) 3.4 Spatial Transformation Next, we check the Coordinate Reference System for our data. Are the coordinate systems for clinic points and community areas the same? For R to treat both coordinate reference systems the same, the metadata has to be exact. st_crs(MetClinics) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] st_crs(Areas) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] We can see that while both have a code of 4326 and appear to both be WGS84 systems, they are not encoded in exactly the same why. Thus, R will treat them differently – which will pose problems for spatial analysis that interacts these two layers. One way of resolving this challenge is to transform the spatial reference system so that they are exact. To complicate matters, we are also interested in generating a buffer to approximate a “service area” around each methadone provider. If we want to use a buffer of a mile, we will need to use a spatial data reference system that uses an appropriate distance metric, like feet or meters. As noted in the previous tutorial the WGS84 coordinate reference system uses degrees, and is not an appropriate CRS for the spatial analysis we require. Thus, our next goal is to transform both spatial data files into a new, standardized CRS. 3.4.1 Transform CRS To calculate buffers, we will need to convert to a different CRS that preserves distance. Trying using a search engine like Google with search terms “CRS Illinois ft”, for example, to look for a code that provides what we need. After searching, we found EPSG:3435 uses feet for a distance metric. We’ll use that! First, set a new CRS. CRS.new &lt;- st_crs(&quot;EPSG:3435&quot;) Next, transform both datasets to your new CRS. MetClinics.3435 &lt;- st_transform(MetClinics, CRS.new) Areas.3435 &lt;- st_transform(Areas, CRS.new) Check the CRS of both datasets again. If they are identical you’re ready to move onto the next step! 3.5 Generate Buffers Now we are ready to generate buffers! We will create a 1-mile buffer to approximate a service area for an urban area. When choosing an appropriate buffer, consider the conceptual model driving your decision. It’s recommended to review literature on common thresholds, consult patients on how they commonly access services, and consider varying travel modes. We choose a mile as a walkable distance for urban environments, commonly used for acceptable distance to travel for grocery stores in cities. Because methadone providers may be utilized as often as grocery stores for some patients, it may be a reasonable start for analysis. We use the st_buffer function to create a buffer, and use 5280 feet to equal one mile. MetClinic_buffers &lt;- st_buffer(MetClinics.3435, 5280) Inspect the structure of the object you just created. Note that this is a new data object, represented as multiple polygons (rather than multiple points). Each buffer around each point is a separate entity. 3.5.1 Visualize buffers Always visualize a spatial object when calculating one, to confirm it worked correctly. If your buffers are much larger or smaller than expected, it’s often a case of mistaken CRS or projection. Retransform your data, and try again. We use tmap again, in the static plot mode. We layer our zip code areas, then providers, and then finally the buffers. We use red to color clinics, and blue to color buffers. tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tm_shape(Areas.3435) + tm_borders() + tm_shape(MetClinics.3435) + tm_dots(col = &quot;red&quot;) + tm_shape(MetClinic_buffers) + tm_borders(col = &quot;blue&quot;) While this map shows our buffers were calculated correctly, the default settings make it difficult to view. To improve aesthetics we change the transparency of zip code boundaries by adjusting the alpha level. We add a fill to the buffers, and make it transparent. We increase the size of the points. # Map Housing Buffers tm_shape(Areas) + tm_borders(alpha = 0.6) + tm_shape(MetClinic_buffers) + tm_fill(col = &quot;blue&quot;, alpha = .4) + tm_borders(col = &quot;blue&quot;) + tm_shape(MetClinics.3435) + tm_dots(col = &quot;red&quot;, size = 0.2) 3.5.2 Buffer union While individual buffers are interesting and can be useful to consider overlapping service areas, we are also interested in getting a sense of which areas fall within a 1-mile service area in our study region – or not. For this, we need to to use a union spatial operation. This will flatten all the individual buffers into one entity. union.buffers &lt;- st_union(MetClinic_buffers) Inspect the data structures of MetClinic_buffers and union.buffers to see what happens to the data in this step. Finally, we map the buffer union. tm_shape(Areas) + tm_borders()+ tm_shape(union.buffers) + tm_fill(col = &quot;blue&quot;, alpha = .2) + tm_borders(col = &quot;blue&quot;) + tm_shape(MetClinics.3435) + tm_dots(col = &quot;red&quot;, size = 0.4) 3.5.3 Save Data We will save the merged 1-mile buffers to bring into maps for future analysis. The st_write function does the trick. Uncomment, and run on your system! #st_write(union.buffers, &quot;data/methadoneClinics_1mi.shp&quot;) 3.6 Rinse &amp; Repeat From here, we can generate additional buffers to compare access associations and generate multiple visuals. We generate a two-mile buffer to add: MetClinic_2mbuffers &lt;- st_buffer(MetClinics.3435, 10560) And then leverage tmap parameter specifications to further customize the a map showing multiple buffers. Here, we add the City of Chicago boundary and soften the zip code boundaries. We add a bounding box for the first zip code layer, so that the whole map is centered around the city boundary (even those the zip codes are layered first). We adjust the transparency of the buffer fills, use different colors, and adjust borders to make the visuals pop. We use the tmap_layout function to take away the frame, add and position a title. Explore the tmap documentation further to find additional options for legends and more. To find color options in R, there are multiple guides online (like this one). tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tm_shape(Areas, bbox=CityBoundary) + tm_borders(alpha = 0.2) + tm_shape(CityBoundary) + tm_borders(lwd = 1.5) + tm_shape(MetClinic_2mbuffers) + tm_fill(col = &quot;gray10&quot;, alpha = .4) + tm_borders(col = &quot;dimgray&quot;, alpha = .4) + tm_shape(MetClinic_buffers) + tm_fill(col = &quot;gray90&quot;, alpha = .4) + tm_borders(col = &quot;darkslategray&quot;) + tm_shape(MetClinics.3435) + tm_dots(col = &quot;red&quot;, size = 0.2) + tm_layout(main.title = &quot;Methadone Clinic Service Areas in Chicago&quot;, main.title.position = &quot;center&quot;, main.title.size = 1, frame = FALSE) Next, we’ll try an interactive map to better explore the data that we have. We switch the tmap_mode to “view” and focus on our merged 1-mile buffer service areas. We add labels for zip codes using the tm_text parameter, and adjust the size. The resulting map lets us zoom and out to explore the data. Clicking on a point will give additional details about the Methadone provider. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(Areas) + tm_borders(alpha = 0.5, col=&quot;gray&quot;)+ tm_text(&quot;GEOID10&quot;, size = 0.7) + tm_shape(CityBoundary) + tm_borders() + tm_shape(union.buffers) + tm_fill(col = &quot;blue&quot;, alpha = .2) + tm_borders(col = &quot;blue&quot;) + tm_shape(MetClinics.3435) + tm_dots(col = &quot;red&quot;) "],["link-contextual-data.html", "4 Link Community Data 4.1 Overview 4.2 Environment Setup 4.3 Clean &amp; Merge Data 4.4 Visualize Data 4.5 Save Data", " 4 Link Community Data 4.1 Overview Geographic location can serve as a “key” that links different datasets together. By referencing each dataset and enabling its spatial location, we can integrate different types of information in one setting. In this tutorial, we will use the approximated “service areas” generated in our buffer analysis to identify vulnerable populations during the COVID pandemic. We will connect Chicago COVID-19 Case data by ZIP Code, available as a flat file on the city’s data portal, to our environment. We will then overlap the 1-mile buffers representing walkable access to the Methadone providers in the city. We use a conservative threshold because of the multiple challenges posed by the COVID pandemic that may impact travel. Our final goal will be to identify zip codes most impacted by COVID that are outside our acceptable access threshold. Our tutorial objectives are to: Clean data in preparation of merge Integrate data using geographic identifiers Generate maps for a basic gap analysis 4.2 Environment Setup To replicate the codes &amp; functions illustrated in this tutorial, you’ll need to have R and RStudio downloaded and installed on your system. This tutorial assumes some familiarity with the R programming language. 4.2.1 Input/Output Our inputs include multiple CSV and SHP files, all of which can be found here, though the providers point file was generated in the Geocoding tutorial. Note that all four files are required (.dbf, .prj, .shp, and .shx) to constitute a shapefile. Chicago Methadone Clinics, methadoneClinics.shp 1-mile buffer zone of Clinics, methadoneClinics_1mi.shp Chicago Zip Codes, chicago_zips.shp Chicago COVID case data by Zip, COVID-19_Cases__Tests__and_Deaths_by_ZIP_Code.csv We will calculate the minimum distance between the resources and the centroids of the zip codes, then save the results as a shapefile and as a CSV. Our final result will be a shapefile/CSV with the minimum distance value for each zip. 4.2.2 Load Libraries We will use the following packages in this tutorial: sf: to manipulate spatial data tmap: to visualize and create maps units: to convert units within spatial data Load the libraries for use. library(sf) library(tmap) 4.2.3 Load data First we’ll load the shapefiles. chicago_zips &lt;- st_read(&quot;data/chicago_zips.shp&quot;) ## Reading layer `chicago_zips&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/data/chicago_zips.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 85 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.06058 ymin: 41.58152 xmax: -87.52366 ymax: 42.06504 ## geographic CRS: WGS 84 meth_sf &lt;- st_read(&quot;data/methadoneClinics.shp&quot;) ## Reading layer `methadoneClinics&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/data/methadoneClinics.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 27 features and 8 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -87.7349 ymin: 41.68698 xmax: -87.57673 ymax: 41.96475 ## geographic CRS: WGS 84 buffers&lt;- st_read(&quot;data/methadoneClinics_1mi.shp&quot;) ## Reading layer `methadoneClinics_1mi&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/data/methadoneClinics_1mi.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 1 feature and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 1141979 ymin: 1824050 xmax: 1195960 ymax: 1935751 ## projected CRS: NAD83 / Illinois East (ftUS) Next, we’ll load some new data we’re interested in joining in: Chicago COVID-19 Cases, Tests, and Deaths by ZIP Code, found on the city data portal here. We’ll load in a CSV and inspect the data: COVID &lt;- read.csv(&quot;data/COVID-19_Cases__Tests__and_Deaths_by_ZIP_Code.csv&quot;) 4.3 Clean &amp; Merge Data First, let’s inspect COVID case data. What information do we need from this file? We may not need everything, so consider just identifying the field with the geographic identifier and main variable(s) of interest. head(COVID) ## ZIP.Code Week.Number Week.Start Week.End Cases...Weekly ## 1 60603 39 09/20/2020 09/26/2020 0 ## 2 60604 39 09/20/2020 09/26/2020 0 ## 3 60611 16 04/12/2020 04/18/2020 8 ## 4 60611 15 04/05/2020 04/11/2020 7 ## 5 60615 11 03/08/2020 03/14/2020 NA ## 6 60603 10 03/01/2020 03/07/2020 NA ## Cases...Cumulative Case.Rate...Weekly Case.Rate...Cumulative ## 1 13 0 1107.3 ## 2 31 0 3964.2 ## 3 72 25 222.0 ## 4 64 22 197.4 ## 5 NA NA NA ## 6 NA NA NA ## Tests...Weekly Tests...Cumulative Test.Rate...Weekly ## 1 25 327 2130 ## 2 12 339 1534 ## 3 101 450 312 ## 4 59 349 182 ## 5 6 9 14 ## 6 0 0 0 ## Test.Rate...Cumulative Percent.Tested.Positive...Weekly ## 1 27853.5 0.0 ## 2 43350.4 0.0 ## 3 1387.8 0.1 ## 4 1076.3 0.1 ## 5 21.7 NA ## 6 0.0 NA ## Percent.Tested.Positive...Cumulative Deaths...Weekly ## 1 0.0 0 ## 2 0.1 0 ## 3 0.2 0 ## 4 0.2 0 ## 5 NA 0 ## 6 NA 0 ## Deaths...Cumulative Death.Rate...Weekly Death.Rate...Cumulative ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## Population Row.ID ZIP.Code.Location ## 1 1174 60603-39 POINT (-87.625473 41.880112) ## 2 782 60604-39 POINT (-87.629029 41.878153) ## 3 32426 60611-16 POINT (-87.620291 41.894734) ## 4 32426 60611-15 POINT (-87.620291 41.894734) ## 5 41563 60615-11 POINT (-87.602725 41.801993) ## 6 1174 60603-10 POINT (-87.625473 41.880112) From this we can assess the need for the following variables: ZIP Code and Percent Tested Positive - Cumulative. Let’s subset the data accordingly. Because this data file has extremely long header names (common in the epi world), let’s use the colnames function to get exactly what we need. colnames(COVID) ## [1] &quot;ZIP.Code&quot; ## [2] &quot;Week.Number&quot; ## [3] &quot;Week.Start&quot; ## [4] &quot;Week.End&quot; ## [5] &quot;Cases...Weekly&quot; ## [6] &quot;Cases...Cumulative&quot; ## [7] &quot;Case.Rate...Weekly&quot; ## [8] &quot;Case.Rate...Cumulative&quot; ## [9] &quot;Tests...Weekly&quot; ## [10] &quot;Tests...Cumulative&quot; ## [11] &quot;Test.Rate...Weekly&quot; ## [12] &quot;Test.Rate...Cumulative&quot; ## [13] &quot;Percent.Tested.Positive...Weekly&quot; ## [14] &quot;Percent.Tested.Positive...Cumulative&quot; ## [15] &quot;Deaths...Weekly&quot; ## [16] &quot;Deaths...Cumulative&quot; ## [17] &quot;Death.Rate...Weekly&quot; ## [18] &quot;Death.Rate...Cumulative&quot; ## [19] &quot;Population&quot; ## [20] &quot;Row.ID&quot; ## [21] &quot;ZIP.Code.Location&quot; 4.3.1 Subset Data We can now subset to just include the fields we need. There are many different ways to subset in R – we just use one example here! Inspect your data to confirm it was pulled correctly. COVID.sub &lt;- COVID[, c(&quot;ZIP.Code&quot;, &quot;Case.Rate...Cumulative&quot;)] head(COVID.sub) ## ZIP.Code Case.Rate...Cumulative ## 1 60603 1107.3 ## 2 60604 3964.2 ## 3 60611 222.0 ## 4 60611 197.4 ## 5 60615 NA ## 6 60603 NA 4.3.2 Identify Geographic Key Before merging, we need to first identify the geographic identifier we would like to merge on. It is the field “ZIP.Code” in our subset. What about the zip code file, which we will be merging to? head(chicago_zips) ## Simple feature collection with 6 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.06058 ymin: 41.73452 xmax: -87.58209 ymax: 42.04052 ## geographic CRS: WGS 84 ## ZCTA5CE10 GEOID10 CLASSFP10 MTFCC10 FUNCSTAT10 ALAND10 ## 1 60501 60501 B5 G6350 S 12532295 ## 2 60007 60007 B5 G6350 S 36493383 ## 3 60651 60651 B5 G6350 S 9052862 ## 4 60652 60652 B5 G6350 S 12987857 ## 5 60653 60653 B5 G6350 S 6041418 ## 6 60654 60654 B5 G6350 S 1464813 ## AWATER10 INTPTLAT10 INTPTLON10 ## 1 974360 +41.7802209 -087.8232440 ## 2 917560 +42.0086000 -087.9973398 ## 3 0 +41.9020934 -087.7408565 ## 4 0 +41.7479319 -087.7147951 ## 5 1696670 +41.8199645 -087.6059654 ## 6 113471 +41.8918225 -087.6383036 ## geometry ## 1 MULTIPOLYGON (((-87.86289 4... ## 2 MULTIPOLYGON (((-88.06058 4... ## 3 MULTIPOLYGON (((-87.77559 4... ## 4 MULTIPOLYGON (((-87.74205 4... ## 5 MULTIPOLYGON (((-87.62623 4... ## 6 MULTIPOLYGON (((-87.64775 4... Aha – in this dataset, the zip is identified as either the ZCTA5CE10 field or GEOID10 field. Not that we are actually working with 5-digit ZCTA fields, not 9-digit ZIP codes… We decide to merge on the GEOID10 field. To make our lives easier, we’ll generate a duplicate field in our subset with a new name, GEOID10, to match. We also convert from the factor structure to a character field to match the data structure of the master zip file. COVID.sub$GEOID10&lt;- as.character(COVID.sub$ZIP.Code) 4.3.3 Merge Data Let’s merge the data using the zip code geographic identifier, “ZIP Code” field, to bring in the the Percent Tested Positive - Cumalative dataset. Inspect the data to confirm it merged correctly! zips_merged &lt;- merge(chicago_zips, COVID.sub, by = &quot;GEOID10&quot;) head(zips_merged) ## Simple feature collection with 6 features and 11 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -87.63396 ymin: 41.88083 xmax: -87.6129 ymax: 41.88893 ## geographic CRS: WGS 84 ## GEOID10 ZCTA5CE10 CLASSFP10 MTFCC10 FUNCSTAT10 ALAND10 AWATER10 ## 1 60601 60601 B5 G6350 S 934226 60682 ## 2 60601 60601 B5 G6350 S 934226 60682 ## 3 60601 60601 B5 G6350 S 934226 60682 ## 4 60601 60601 B5 G6350 S 934226 60682 ## 5 60601 60601 B5 G6350 S 934226 60682 ## 6 60601 60601 B5 G6350 S 934226 60682 ## INTPTLAT10 INTPTLON10 ZIP.Code Case.Rate...Cumulative ## 1 +41.8856419 -087.6215226 60601 1451.4 ## 2 +41.8856419 -087.6215226 60601 872.2 ## 3 +41.8856419 -087.6215226 60601 919.9 ## 4 +41.8856419 -087.6215226 60601 558.8 ## 5 +41.8856419 -087.6215226 60601 156.7 ## 6 +41.8856419 -087.6215226 60601 477.0 ## geometry ## 1 MULTIPOLYGON (((-87.63396 4... ## 2 MULTIPOLYGON (((-87.63396 4... ## 3 MULTIPOLYGON (((-87.63396 4... ## 4 MULTIPOLYGON (((-87.63396 4... ## 5 MULTIPOLYGON (((-87.63396 4... ## 6 MULTIPOLYGON (((-87.63396 4... 4.4 Visualize Data Now we are ready to visualize our data! First we’ll make a simple map. We generate a choropleth map of case rate data using quantile bins, and the Blue-Purple color palette. You can find an R color cheatsheet useful for identifying palette codes here. (More details on thematic mapping are in tutorials that follow!) We overlay the buffers and clincal providers. tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tm_shape(zips_merged) + tm_polygons(&quot;Case.Rate...Cumulative&quot;, style=&quot;quantile&quot;, pal=&quot;BuPu&quot;, title = &quot;COVID Case Rate&quot;) + tm_shape(buffers) + tm_borders(col = &quot;blue&quot;) + tm_shape(meth_sf) + tm_dots(col = &quot;black&quot;, size = 0.2) Already we can generate some insight. Areas on the far West side of the city have some of the highest case rates, but are outside a walkable distance to Methadone providers. For individuals with opioid use disorder requiring medication access in these locales, they may be especially vulnerable during the pandemic. Next, we adjust some tmap parameters to improve our map. Now we switch to a red-yellow-green palette, and specify six bins for our quantile map. We flip the direction of the palette using a negative sign, so that red corresponds to areas with higher rates. We adjust transparency using an alpha parameter, and line thickness using the lwd parameter. tm_shape(zips_merged) + tm_fill(&quot;Case.Rate...Cumulative&quot;, style=&quot;quantile&quot;, n=6, pal=&quot;-RdYlGn&quot;, title = &quot;COVID Case Rate&quot;,alpha = 0.8) + tm_borders(lwd = 0) + tm_shape(buffers) + tm_borders(col = &quot;gray&quot;) + tm_fill(alpha=0.5) + tm_shape(meth_sf) + tm_dots(col = &quot;black&quot;, size = 0.1) + tm_layout(main.title = &quot;Walkable Methadone Service Areas&quot;, main.title.position = &quot;center&quot;, main.title.size = 1, frame = FALSE) To improve our map even further, let’s make in interactive! By switching the tmap_mode function to “view” (from “plot” the default), our newly rendered map is not interactive. We can zoom in and out, click on different basemaps or turn layers on our off, and click on resources for more information. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(zips_merged) + tm_fill(&quot;Case.Rate...Cumulative&quot;, style=&quot;quantile&quot;, n=6, pal=&quot;-RdYlGn&quot;, title = &quot;COVID Case Rate&quot;,alpha = 0.8) + tm_borders(lwd = 0) + tm_shape(buffers) + tm_borders(col = &quot;gray&quot;) + tm_fill(alpha=0.5) + tm_shape(meth_sf) + tm_dots(col = &quot;black&quot;, size = 0.1) Using this approach, it’s clear that the zip codes on the West Side most vulnerable are 60651, 60644, 60632, and 60629. By updating the thresholds and parameters further, these can shift as well to be more or less conservative based on our assumptions. 4.5 Save Data We save our newly merged ZCTA level data for future analysis. write_sf(zips_merged, &quot;data/chizips_COVID.shp&quot;) ## Warning in abbreviate_shapefile_names(obj): Field names ## abbreviated for ESRI Shapefile driver "],["getACSData-tutorial.html", "5 Census Data Wrangling 5.1 Overview 5.2 Environment Setup 5.3 Enable Census API Key 5.4 Load Data Dynamically 5.5 Get Geometry 5.6 Appendix", " 5 Census Data Wrangling 5.1 Overview Once we identify the appropriate access metric to use, we can now include contextual data to add nuance to our findings. This can help identify if any specific disparities in access exist for certain groups of people or if there are any specific factors that can help explain the spatial patterns. Such datasets are often sourced from the US Census Bureau. The American Community Survey (ACS) is an ongoing survey that provides data every year with 1 and 5-year estimates. We generally recommend using the 5-year estimates as these multiperiod estimates tend to have increased statistical reliability as compared to the 1-year numbers, especially for less populated areas and small population subgroups. In this tutorial we demonstrate how to explore and download most commonly used population datasets from the same, with and without spatial components. Please note this tutorial focuses only on the American Community Survey datasets available via the Census Bureau API. Our objectives are to: Download census data through the Census API Download census boundaries thorough the Census API Wrangle 5.2 Environment Setup To replicate the codes &amp; functions illustrated in this tutorial, you’ll need to have R and RStudio downloaded and installed on your system. This tutorial assumes some familiarity with the R programming language. 5.2.1 Input/Output We will not be using an external input for this exercise. Our output will be two sets of files: CSV file and shapefile with Race Data distributions at county level for the state of Illinois. and CSV file and shapefile with Population and Per Capita Income for the zipcodes within the city of Chicago for 2018. 5.2.2 Load Libraries We will use the following packages in this tutorial: sf: to read/write sf (spatial) objects tidycensus: to download census variables using ACS API tidyverse: to manipulate and clean data tigris : to download census tiger shapefiles Load the required libraries. library(sf) library(tidycensus) library(tidyverse) library(tigris) 5.3 Enable Census API Key To be able to use the Census API, we need to signup for an API key. This key effectively is a string identifier for the server to communicate with your machine. A key can be obtained using an email from here. Once we get the key, we can install it by running the code below. census_api_key(&quot;yourkeyhere&quot;, install = TRUE) # installs the key for future sessions. In instances where we might not want to save our key in the .Renviron - for example, when using a shared computer, we can always reinstall the same key using the code above but with install = FALSE. To check an already installed census API key, run Sys.getenv(&quot;CENSUS_API_KEY&quot;) 5.4 Load Data Dynamically We can now start using the tidycensus package to download population based datasets from the US Census Bureau. In this tutorial, we will be covering methods to download data at the state, county, zip and census tract levels. We will also be covering methods to download the data with and without the geometry feature of the geographic entities. To download a particular variable or table using tidycensus, we need the relevant variable ID, which one can check by reviewing the variables available via load_variables() function. For details on exploring the variables available via the tidycensus &amp; to get their identifiers, check the Explore variables available section in Appendix. We can now download the variables using get_acs() function. Given ACS data is based of an annual sample, the datapoints are available as an estimate with a margin or error (moe). The package provides both values for any requested variable in the tidy format. For the examples covered in this tutorial, the 4 main inputs for get_acs() function are: geography - for what scale to source the data for (state / county / tract / zcta) variables - character string or a vector of character strings of variable IDs to source year - the year to source the data for geometry - whether or not to include the geometry feature in the tibble. (TRUE / FALSE) 5.4.1 State Level To get data for only a specific state, we can add state = sampleStateName. stateDf &lt;- get_acs(geography = &#39;state&#39;, variables = c(totPop18 = &quot;B01001_001&quot;, hispanic =&quot;B03003_003&quot;, notHispanic = &quot;B03003_002&quot;, white = &quot;B02001_002&quot;, afrAm = &quot;B02001_003&quot;, asian = &quot;B02001_005&quot;), year = 2018, geometry = FALSE) head(stateDf) ## # A tibble: 6 x 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alabama totPop18 4864680 NA ## 2 01 Alabama white 3317453 3345 ## 3 01 Alabama afrAm 1293186 2745 ## 4 01 Alabama asian 64609 1251 ## 5 01 Alabama notHispanic 4661534 393 ## 6 01 Alabama hispanic 203146 393 As we can see the data is available in the tidy format. We can use other tools in the tidyverse universe to clean and manipulate it. stateDf &lt;- stateDf %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% spread(variable, estimate) %&gt;% mutate(hispPr18 = hispanic/totPop18, WhitePr18 = white/totPop18, AfrAmPr18 = afrAm/totPop18, AsianPr18 = asian/totPop18) %&gt;% select(GEOID,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) head(stateDf) ## # A tibble: 6 x 6 ## GEOID totPop18 hispPr18 WhitePr18 AfrAmPr18 AsianPr18 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 4864680 0.0418 0.682 0.266 0.0133 ## 2 02 738516 0.0693 0.648 0.0327 0.0630 ## 3 04 6946685 0.311 0.772 0.0439 0.0329 ## 4 05 2990671 0.0732 0.770 0.154 0.0147 ## 5 06 39148760 0.389 0.601 0.0579 0.143 ## 6 08 5531141 0.214 0.842 0.0412 0.0312 5.4.2 County Level Similarly, for county level use geometry = county to download for all counties in the U.S. use geometry = county, state = sampleStateName for all counties within a state use geometry = county, state = sampleStateName, county = sampleCountyName for a specific county countyDf &lt;- get_acs(geography = &#39;county&#39;, variables = c(totPop18 = &quot;B01001_001&quot;, hispanic =&quot;B03003_003&quot;, notHispanic = &quot;B03003_002&quot;, white = &quot;B02001_002&quot;, afrAm = &quot;B02001_003&quot;, asian = &quot;B02001_005&quot;), year = 2018, state = &#39;IL&#39;, geometry = FALSE) %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% spread(variable, estimate) %&gt;% mutate(hispPr18 = hispanic/totPop18, WhitePr18 = white/totPop18, AfrAmPr18 = afrAm/totPop18, AsianPr18 = asian/totPop18) %&gt;% select(GEOID,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) head(countyDf) ## # A tibble: 6 x 6 ## GEOID totPop18 hispPr18 WhitePr18 AfrAmPr18 AsianPr18 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17001 66427 0.0154 0.931 0.0408 0.00813 ## 2 17003 6532 0.0112 0.624 0.332 0.000919 ## 3 17005 16712 0.0346 0.909 0.0624 0.0117 ## 4 17007 53606 0.214 0.874 0.0222 0.0118 ## 5 17009 6675 0.0428 0.774 0.204 0.00554 ## 6 17011 33381 0.0897 0.936 0.00932 0.00866 And now we can save the dataset in CSV file using code below. write.csv(countyDf, &quot;data/IL_County_Race_18.csv&quot;) 5.4.3 Census Tract Level For census tract level, at the minimum stateName needs to be provided. use geometry = tract, state = sampleStateName to download all tracts within a state use geometry = tract, state = sampleStateName, county = sampleCountyName to download all tracts within a specific county tractDf &lt;- get_acs(geography = &#39;tract&#39;,variables = c(totPop18 = &quot;B01001_001&quot;, hispanic =&quot;B03003_003&quot;, notHispanic = &quot;B03003_002&quot;, white = &quot;B02001_002&quot;, afrAm = &quot;B02001_003&quot;, asian = &quot;B02001_005&quot;), year = 2018, state = &#39;IL&#39;, geometry = FALSE) %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% spread(variable, estimate) %&gt;% mutate(hispPr18 = hispanic/totPop18, WhitePr18 = white/totPop18, AfrAmPr18 = afrAm/totPop18, AsianPr18 = asian/totPop18) %&gt;% select(GEOID,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) head(tractDf) 5.4.4 Zipcode Level For zipcode level, use geometry = zcta. Given zips cross county/state lines, zcta data is only available for the entire U.S. zctaDf &lt;- get_acs(geography = &#39;zcta&#39;,variables = c(totPop18 = &quot;B01001_001&quot;, hispanic =&quot;B03003_003&quot;, notHispanic = &quot;B03003_002&quot;, white = &quot;B02001_002&quot;, afrAm = &quot;B02001_003&quot;, asian = &quot;B02001_005&quot;), year = 2018, geometry = FALSE) %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% spread(variable, estimate) %&gt;% mutate(hispPr18 = hispanic/totPop18, WhitePr18 = white/totPop18, AfrAmPr18 = afrAm/totPop18, AsianPr18 = asian/totPop18) %&gt;% select(GEOID,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) Inspect the data. head(zctaDf) ## # A tibble: 6 x 6 ## GEOID totPop18 hispPr18 WhitePr18 AfrAmPr18 AsianPr18 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00601 17242 0.997 0.755 0.00841 0.000174 ## 2 00602 38442 0.935 0.794 0.0278 0 ## 3 00603 48814 0.974 0.765 0.0395 0.00746 ## 4 00606 6437 0.998 0.408 0.0231 0 ## 5 00610 27073 0.962 0.755 0.0257 0 ## 6 00612 60303 0.993 0.807 0.0456 0.00985 dim(zctaDf) ## [1] 33120 6 Given zipcode data can only be sourced for the entire nation, after sourcing it, we can filter them for certain region,e.g. below we can filter for zipcodes in Chicago by using str_detect and then save it to a .csv file. zipChicagoDf &lt;- get_acs(geography = &#39;zcta&#39;, variables = c(perCapInc = &quot;DP03_0088&quot;),year = 2018, geometry = FALSE) %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% filter(str_detect( GEOID,&quot;^606&quot;)) %&gt;% spread(variable, estimate) %&gt;% select(GEOID, perCapInc) ## Getting data from the 2014-2018 5-year ACS ## Using the ACS Data Profile write.csv(zipChicagoDf , file = &quot;data/Chicago_Zip_PCI_18.csv&quot;) For more details on the other geographies available via the tidycensus package, check here. 5.5 Get Geometry Geometry/Geographic Boundaries are one of the key features for American Community Survey Data as they set up the framework for data collection and estimation. While boundaries don’t change often, updates do occur from time to time and census data for a specific year generally tends to use the boundaries available at the beginning of that year. Most ACS products since 2010 reflect the 2010 Census Geographic Definitions. Given certain boundaries like congressional districts, census tracts &amp; block groups are updated after every decennial census, products for year 2009 and earlier will have significantly different boundaries from that in 2010. We recommend using IPUMS datasets to generate estimates for years prior to 2010. The datasets downloaded so far did not have a spatial geometry feature attached to them. To run any spatial analysis on the race data above, we would need to join these dataframes to another spatially-enabled sf object. We can do so by joining on the ‘GEOID’ or any other identifier. We can download the geometry information using two methods : using tigris using tidycensus 5.5.1 Using tigris To download and use the Tiger Shapefiles shared by the US Census Bureau we will use the tigris package. Set cb = TRUE to get generalized files, these don’t have high resolution details and hence are smaller in size. yeartoFetch &lt;- 2018 stateShp &lt;- states(year = yeartoFetch, cb = TRUE) countyShp &lt;- counties(year = yeartoFetch, state = &#39;IL&#39;, cb = TRUE) tractShp &lt;- tracts(state = &#39;IL&#39;,year = yeartoFetch, cb = TRUE) zctaShp &lt;- zctas(year = yeartoFetch, cb = TRUE) Now we can merge these geometry files with the race data downloaded in previous section. For states: # check object types &amp; identifier variable type # str(stateShp) # str(stateDf) stateShp &lt;- merge(stateShp, stateDf, by.x = &#39;STATEFP&#39;, by.y = &#39;GEOID&#39;, all.x = TRUE) head(stateShp) ## Simple feature collection with 6 features and 14 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -179.1489 ymin: 30.22333 xmax: 179.7785 ymax: 71.36516 ## geographic CRS: NAD83 ## STATEFP STATENS AFFGEOID GEOID STUSPS NAME LSAD ## 1 01 01779775 0400000US01 01 AL Alabama 00 ## 2 02 01785533 0400000US02 02 AK Alaska 00 ## 3 04 01779777 0400000US04 04 AZ Arizona 00 ## 4 05 00068085 0400000US05 05 AR Arkansas 00 ## 5 06 01779778 0400000US06 06 CA California 00 ## 6 08 01779779 0400000US08 08 CO Colorado 00 ## ALAND AWATER totPop18 hispPr18 WhitePr18 ## 1 1.311740e+11 4593327154 4864680 0.04175938 0.6819468 ## 2 1.478840e+12 245481577452 738516 0.06930926 0.6483732 ## 3 2.941986e+11 1027337603 6946685 0.31141645 0.7721872 ## 4 1.347689e+11 2962859592 2990671 0.07324510 0.7700192 ## 5 4.035039e+11 20463871877 39148760 0.38881377 0.6010169 ## 6 2.684229e+11 1181621593 5531141 0.21420427 0.8417041 ## AfrAmPr18 AsianPr18 geometry ## 1 0.26583167 0.01328124 MULTIPOLYGON (((-88.05338 3... ## 2 0.03267228 0.06303993 MULTIPOLYGON (((179.4825 51... ## 3 0.04394312 0.03294910 MULTIPOLYGON (((-114.8163 3... ## 4 0.15413598 0.01470840 MULTIPOLYGON (((-94.61783 3... ## 5 0.05792968 0.14315496 MULTIPOLYGON (((-118.6044 3... ## 6 0.04120994 0.03122231 MULTIPOLYGON (((-109.0603 3... Similarly for counties, zctas &amp; census tracts we can use the code below and save the shapefiles. countyShp &lt;- merge(countyShp, countyDf, by.x = &#39;GEOID&#39;, by.y = &#39;GEOID&#39;, all.x = TRUE) %&gt;% select(GEOID,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) tractShp &lt;- merge(tractShp, tractDf, by.x = &#39;GEOID&#39;, by.y = &#39;GEOID&#39;, all.x = TRUE) %&gt;% select(GEOID,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) zctaShp &lt;- merge(zctaShp, zctaDf, by.x = &#39;GEOID10&#39;, by.y = &#39;GEOID&#39;, all.x = TRUE)%&gt;% select(GEOID10,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) write_sf(countyShp, &quot;data/Chicago_Zip_PCI_18.shp&quot;) Now we can finally filter the zipcodes for Chicago and save the census tract results with geometry in a shapefile using write_sf. zipChicagoShp &lt;- merge(zctaShp %&gt;% filter(str_detect(GEOID10,&quot;^606&quot;)), zipChicagoDf, by.x = &#39;GEOID10&#39;, by.y = &#39;GEOID&#39;, all.x = TRUE) %&gt;% select(GEOID10,totPop18,perCapInc) write_sf(zipChicagoShp, &quot;data/Chicago_Zip_PCI_18.shp&quot;) 5.5.2 Using tidycensus The previous method adds an additional step of using tigris package to download the shapefile. The tidycensus package already has the wrapper for invoking tigris within the get_acs() function, and we can simply download the dataset with geometry feature by using geometry = TRUE. The wrapper adds the geometry information to each variable sourced, and the file size can become large in the intermediary steps and slow down the performance, even though the data is in tidy format. So if you are looking to download many variables with large API requests, we recommend downloading the dataset without geometry information and then downloading a nominal variable like total population or per capita income with get geometry using get_acs() or simply using the tigris method, as covered in previous section &amp; then implementing a merge. We have illustrated both methods below. tractDf &lt;- get_acs(geography = &#39;tract&#39;, variables = c(totPop18 = &quot;B01001_001&quot;, hispanic =&quot;B03003_003&quot;, notHispanic = &quot;B03003_002&quot;, white = &quot;B02001_002&quot;, afrAm = &quot;B02001_003&quot;, asian = &quot;B02001_005&quot;), year = 2018, state = &#39;IL&#39;, geometry = FALSE) %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% spread(variable, estimate) %&gt;% mutate(hispPr18 = hispanic/totPop18, WhitePr18 = white/totPop18, AfrAmPr18 = afrAm/totPop18, AsianPr18 = asian/totPop18) %&gt;% select(GEOID,totPop18,hispPr18,WhitePr18,AfrAmPr18, AsianPr18) tractShp &lt;- get_acs(geography = &#39;tract&#39;, variables = c(perCapitaIncome = &quot;DP03_0088&quot;), year = 2018, state = &#39;IL&#39;, geometry = TRUE) %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% spread(variable, estimate) tractsShp &lt;- merge(tractShp, tractDf, by.x = &#39;GEOID&#39;, by.y = &#39;GEOID&#39;, all.x = TRUE) head(tractShp) ## Simple feature collection with 6 features and 3 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.79336 ymin: 41.7943 xmax: -87.63536 ymax: 41.95088 ## geographic CRS: NAD83 ## GEOID NAME ## 1 17031843800 Census Tract 8438, Cook County, Illinois ## 2 17037001002 Census Tract 10.02, DeKalb County, Illinois ## 3 17031243000 Census Tract 2430, Cook County, Illinois ## 4 17031250600 Census Tract 2506, Cook County, Illinois ## 5 17031251700 Census Tract 2517, Cook County, Illinois ## 6 17031260400 Census Tract 2604, Cook County, Illinois ## perCapitaIncome geometry ## 1 19331 MULTIPOLYGON (((-87.64554 4... ## 2 11308 MULTIPOLYGON (((-88.79317 4... ## 3 48843 MULTIPOLYGON (((-87.68195 4... ## 4 22905 MULTIPOLYGON (((-87.7756 41... ## 5 14739 MULTIPOLYGON (((-87.74826 4... ## 6 12610 MULTIPOLYGON (((-87.74061 4... zipChicagoShp &lt;- get_acs(geography = &#39;zcta&#39;, variables = c(totPop18 = &quot;B01001_001&quot;, perCapInc = &quot;DP03_0088&quot;), year = 2018, geometry = TRUE) %&gt;% select(GEOID, NAME, variable, estimate) %&gt;% spread(variable, estimate) %&gt;% rename(totPop18 = B01001_001, perCapitaInc = DP03_0088) %&gt;% filter(str_detect(GEOID,&quot;^606&quot;)) %&gt;% select(GEOID,totPop18,perCapitaInc) head(zipChicagoShp) ## Simple feature collection with 6 features and 3 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -87.64138 ymin: 41.85206 xmax: -87.60586 ymax: 41.88908 ## geographic CRS: NAD83 ## # A tibble: 6 x 4 ## GEOID totPop18 perCapitaInc geometry ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 60601 14675 92125 (((-87.63396 41.88698, -87.63102 41… ## 2 60602 1244 100507 (((-87.63389 41.88447, -87.62536 41… ## 3 60603 1174 117992 (((-87.63382 41.88195, -87.62781 41… ## 4 60604 782 114575 (((-87.63375 41.87939, -87.62428 41… ## 5 60605 27519 83408 (((-87.63311 41.87686, -87.62421 41… ## 6 60606 3101 132765 (((-87.63998 41.88851, -87.63936 41… 5.6 Appendix 5.6.1 Explore variables Using tidycensus we can download datasets from various types of tables. The ones most commonly used are: Data Profiles - These are the most commonly used collection of variables grouped by category, e.g. Social (DP02), Economic (DP03), Housing (DP04), Demographic (DP05) Subject Profiles - These generally have more detailed information variables (than DP) grouped by category, e.g. Age &amp; Sex (S0101), Disability Characteristics (S1810) The package also allows access to a suite of B &amp; C tables. We can explore all the variables for our year of interest by running the code below. Please note as the Profiles evolve, variable IDs might change from year to year. sVarnames &lt;- load_variables(2018, &quot;acs5/subject&quot;, cache = TRUE) pVarnames &lt;- load_variables(2018, &quot;acs5/profile&quot;, cache = TRUE) otherVarnames &lt;- load_variables(2018, &quot;acs5&quot;, cache = TRUE) head(pVarnames) ## # A tibble: 6 x 3 ## name label concept ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DP02_00… Estimate!!HOUSEHOLDS BY TYPE… SELECTED SOCIAL CHARACTE… ## 2 DP02_00… Percent Estimate!!HOUSEHOLDS… SELECTED SOCIAL CHARACTE… ## 3 DP02_00… Estimate!!HOUSEHOLDS BY TYPE… SELECTED SOCIAL CHARACTE… ## 4 DP02_00… Percent Estimate!!HOUSEHOLDS… SELECTED SOCIAL CHARACTE… ## 5 DP02_00… Estimate!!HOUSEHOLDS BY TYPE… SELECTED SOCIAL CHARACTE… ## 6 DP02_00… Percent Estimate!!HOUSEHOLDS… SELECTED SOCIAL CHARACTE… A tibble with table &amp; variable information has three columns : name, label, concept. Name is a combination of table id and variable id within that table. Concept generally identifies the table name or grouping used to arrange variables. Label provides textual details about the variable. We can explore these tibbles to identify the correct variable ID name to use with the get_acs() function by using View(sVarnames) or other filters e.g. for age sVarnames %&gt;% filter(str_detect(concept, &quot;AGE AND SEX&quot;)) %&gt;% # search for this concept filter(str_detect(label, &quot;Under 5 years&quot;)) %&gt;% # search for variables mutate(label = sub(&#39;^Estimate!!&#39;, &#39;&#39;, label)) %&gt;% # remove unnecessary text select(variableId = name, label) # drop unnecessary columns and rename ## # A tibble: 6 x 2 ## variableId label ## &lt;chr&gt; &lt;chr&gt; ## 1 S0101_C01_002 Total!!Total population!!AGE!!Under 5 years ## 2 S0101_C02_002 Percent!!Total population!!AGE!!Under 5 years ## 3 S0101_C03_002 Male!!Total population!!AGE!!Under 5 years ## 4 S0101_C04_002 Percent Male!!Total population!!AGE!!Under 5 years ## 5 S0101_C05_002 Female!!Total population!!AGE!!Under 5 years ## 6 S0101_C06_002 Percent Female!!Total population!!AGE!!Under 5 ye… sVarnames %&gt;% filter(str_sub(name, 1, 5) == &quot;S0101&quot;) %&gt;% # search for these tables filter(str_detect(label, &quot;Under 5 years&quot;)) %&gt;% # search for variables mutate(label = sub(&#39;^Estimate!!&#39;, &#39;&#39;, label)) %&gt;% # remove unnecessary text select(variableId = name, label) # drop unnecessary columns and rename ## # A tibble: 6 x 2 ## variableId label ## &lt;chr&gt; &lt;chr&gt; ## 1 S0101_C01_002 Total!!Total population!!AGE!!Under 5 years ## 2 S0101_C02_002 Percent!!Total population!!AGE!!Under 5 years ## 3 S0101_C03_002 Male!!Total population!!AGE!!Under 5 years ## 4 S0101_C04_002 Percent Male!!Total population!!AGE!!Under 5 years ## 5 S0101_C05_002 Female!!Total population!!AGE!!Under 5 years ## 6 S0101_C06_002 Percent Female!!Total population!!AGE!!Under 5 ye… e.g per capita income, we can check on DP table variables. pVarnames %&gt;% filter(str_detect(label, &quot;Per capita&quot;)) %&gt;% # search for variables mutate(label = sub(&#39;^Estimate!!&#39;, &#39;&#39;, label)) %&gt;% # remove unnecessary text select(variable = name, label) # drop unnecessary columns and rename ## # A tibble: 2 x 2 ## variable label ## &lt;chr&gt; &lt;chr&gt; ## 1 DP03_0088 INCOME AND BENEFITS (IN 2018 INFLATION-ADJUSTED DOLL… ## 2 DP03_0088P Percent Estimate!!INCOME AND BENEFITS (IN 2018 INFLA… pVarnames %&gt;% filter(str_detect(label, &quot;Under 5 years&quot;)) %&gt;% # search for variables mutate(label = sub(&#39;^Estimate!!&#39;, &#39;&#39;, label)) %&gt;% # remove unnecessary text select(variable = name, label) # drop unnecessary columns and rename ## # A tibble: 2 x 2 ## variable label ## &lt;chr&gt; &lt;chr&gt; ## 1 DP05_0005 SEX AND AGE!!Total population!!Under 5 years ## 2 DP05_0005P Percent Estimate!!SEX AND AGE!!Total population!!Und… The order and structure of profile tables can change from year to year, hence the variable Id or label, so when downloading same dataset over different years we recommend using the standard B &amp; C tables. otherVarnames %&gt;% filter(str_detect(label, &quot;Per capita&quot;)) %&gt;% # search for variables mutate(label = sub(&#39;^Estimate!!&#39;, &#39;&#39;, label)) %&gt;% # remove unnecessary text select(variable = name, label) # drop unnecessary columns and rename ## # A tibble: 10 x 2 ## variable label ## &lt;chr&gt; &lt;chr&gt; ## 1 B19301_001 Per capita income in the past 12 months (in 2018 i… ## 2 B19301A_001 Per capita income in the past 12 months (in 2018 i… ## 3 B19301B_001 Per capita income in the past 12 months (in 2018 i… ## 4 B19301C_001 Per capita income in the past 12 months (in 2018 i… ## 5 B19301D_001 Per capita income in the past 12 months (in 2018 i… ## 6 B19301E_001 Per capita income in the past 12 months (in 2018 i… ## 7 B19301F_001 Per capita income in the past 12 months (in 2018 i… ## 8 B19301G_001 Per capita income in the past 12 months (in 2018 i… ## 9 B19301H_001 Per capita income in the past 12 months (in 2018 i… ## 10 B19301I_001 Per capita income in the past 12 months (in 2018 i… "],["visualizeArealData-tutorial.html", "6 Thematic Mapping 6.1 Overview 6.2 Environment Setup 6.3 Load data 6.4 Thematic Plotting 6.5 Appendix", " 6 Thematic Mapping 6.1 Overview Once we have downloaded the contextual data and generated the access metrics, we can start visualizing them to identify any spatial patterns. This can help identify whether a variable is homogeneously distributed across space or do we see clustering &amp; spatial heterogeneity. In this tutorial we will cover methods to plot data variables spatially i.e. create thematic maps, technically known as choropleth maps. We will cover the most commonly used types of choropleth mapping techniques employed in R. Please note the methods covered here are mere an introduction to spatial plotting. Thus, in this tutorial our objectives are to: Plot basic thematic map Visualize spatial distributions using thematic maps 6.2 Environment Setup To replicate the codes &amp; functions illustrated in this tutorial, you’ll need to have R and RStudio downloaded and installed on your system. This tutorial assumes some familiarity with the R programming language. 6.2.0.1 Packages used We will use the following packages in this tutorial: tidyverse: to manipulate data tmap: to visualize and create maps sf: to read/write and manipulate spatial data 6.2.0.2 Required Inputs and Expected Outputs We will using the per capita income data for the City of Chicago downloaded &amp; saved as a shapefile in the Census Data Wrangling. Our inputs will be : a zip code boundary file with per capita income (“Chicago_Zip_PCI_18.shp”). Our output will be three thematic maps highlighting the distribution of per capita income at a zip code level across the city of Chicago. 6.2.0.3 Load libraries First, load the libraries required. library(tidyverse) library(tmap) library(sf) 6.3 Load data We will read in the shapefile with the per capita income at the zipcode level for the city of Chicago for year 2018. chicagoZips &lt;- st_read(&quot;data/Chicago_Zip_PCI_18.shp&quot;) ## Reading layer `Chicago_Zip_PCI_18&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/data/Chicago_Zip_PCI_18.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 56 features and 3 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -87.86962 ymin: 41.62992 xmax: -87.52416 ymax: 42.02313 ## geographic CRS: NAD83 head(chicagoZips) ## Simple feature collection with 6 features and 3 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -87.64138 ymin: 41.85206 xmax: -87.60586 ymax: 41.88908 ## geographic CRS: NAD83 ## GEOID10 totPop18 perCapInc geometry ## 1 60601 14675 92125 MULTIPOLYGON (((-87.63396 4... ## 2 60602 1244 100507 MULTIPOLYGON (((-87.63389 4... ## 3 60603 1174 117992 MULTIPOLYGON (((-87.63382 4... ## 4 60604 782 114575 MULTIPOLYGON (((-87.63375 4... ## 5 60605 27519 83408 MULTIPOLYGON (((-87.63311 4... ## 6 60606 3101 132765 MULTIPOLYGON (((-87.63998 4... Lets review the dataset structure. In the R sf data object, the ‘geometry’ column provides the geographic information/boundaries that we can map. This is unique to simple features data structures, and a pretty phenomenal concept. We can do a quick plot using: plot(chicagoZips$geometry) 6.4 Thematic Plotting We will be using tmap package for plotting spatial data distributions. The package syntax has similarities with ggplot2 and follows the same idea of A Layered Grammar of Graphics. for each input data layer use tm_shape(), followed by the method to plot it, e.g tm_fill() or tm_dots() or tm_line() or tm_borders() etc. Similar to ggplot2, aesthetics can be provided for each layer and plot layout can be manipulated using tm_layout(). For more details on tmap usage &amp; functionality, check tmap documentation. The previous map we plotted using plot can be mapped using tmap as in the code below. tmap_mode(&#39;plot&#39;) ## tmap mode set to plotting tm_shape(chicagoZips) + tm_borders() + tm_layout(frame = FALSE) In tmap, the classification scheme is set by the style option in tm_fill() and the default style is pretty. Lets plot the distribution of per capita income by zipcode across the city of Chicago with default style using the code below. We can also change the color palette used to depict the spatial distribution. See Set Color Palette in Appendix for more details on that. tm_shape(chicagoZips) + tm_fill(&#39;perCapInc&#39;, title = &#39;Per Capita Income - Pretty&#39;) + tm_borders() + tm_layout(frame = FALSE, legend.outside = TRUE, legend.outside.position = &#39;right&#39;, legend.title.size = 0.9, main.title = &#39;Per Capita Income, by Zipcode, Chicago 2018&#39;, main.title.size = 0.9) We will be plotting the spatial distribution of variable perCapIncome for the city of Chicago using three methods. Quantile Natural Breaks Standard Deviation For detailed documentation on choropleth mapping and methods use check GeoDa Center Documentation. 6.4.1 Quantile A quantile map is based on sorted values for the variable that are then grouped into bins such that each bin has the same number of observations. It is obtained by setting style = 'quantile' and n = no of bins arguments in tm_fill(). p1 &lt;- tm_shape(chicagoZips) + tm_fill(&#39;perCapInc&#39;, title = &#39;Per Capita Income - Quantile&#39;, style = &#39;quantile&#39;, n = 5) + tm_borders() + tm_layout(frame = FALSE,legend.outside = TRUE, legend.outside.position = &#39;right&#39;, legend.title.size =0.9, main.title = &#39;Per Capita Income, by Zipcode, Chicago 2018&#39;, main.title.size = 0.9) #tmap_save(p1, &#39;PctHisp_18_Quantile.png&#39;) # save the map in a .png file p1 6.4.2 Natural Breaks Natural breaks or jenks distribution uses a nonlinear algorithm to cluster data into groups such that the intra-bin similarity is maximized and inter-bin dissimilarity is minimized. It is obtained by setting style = 'jenks' and n = no. of bins in the tm_fill(). As we can see, jenks method better classifies the dataset in review than the quantile distribution. There is no correct method to use and the choice of classification method is dependent on the problem &amp; dataset used. p2 &lt;- tm_shape(chicagoZips) + tm_fill(&#39;perCapInc&#39;, title = &#39;Per Capita Income - Jenks&#39;, style = &#39;jenks&#39;, n = 5) + tm_borders() + tm_layout(frame = FALSE,legend.outside = TRUE, legend.outside.position = &#39;right&#39;, legend.title.size =0.9, main.title = &#39;Per Capita Income, by Zipcode, Chicago 2018&#39;, main.title.size = 0.9) #tmap_save(p2, &#39;PctHisp_18_Jenks.png&#39;)# save the map in a .png file p2 6.4.3 Standard Deviation A standard deviation map normalizes the dataset (mean = 0, stdev = 1) and transforms it into units of stdev (given mean =0). It helps identify outliers in the dataset. It is obtained by setting style = 'sd' in the tm_fill(). The normalization process can create bins with negative values, which in this case don’t necessarily make sense for the dataset, but it still helps identify the outliers. p3 &lt;- tm_shape(chicagoZips) + tm_fill(&#39;perCapInc&#39;, title = &#39;Per Capita Income - Stdev&#39;, style = &#39;sd&#39;) + tm_borders() + tm_layout(frame = FALSE, legend.outside = TRUE, legend.outside.position = &#39;right&#39;, legend.title.size =0.9, main.title = &#39;Per Capita Income, by Zipcode, Chicago 2018&#39;, main.title.size = 0.9) #tmap_save(p3, &#39;PctHisp_18_Stdev.png&#39;)# save the map in a .png file p3 6.5 Appendix Set Color Palette The range of colors used to depict the distribution in the map can be set by modifying the palette argument in tm_fill(). For example, we can use Blues palette to create the map below. tm_shape(chicagoZips) + tm_fill(&#39;perCapInc&#39;, title = &#39;Per Capita Income - Jenks&#39;, style = &#39;jenks&#39;, n = 5, palette = &#39;Blues&#39;) + tm_borders() + tm_layout(frame = FALSE,legend.outside = TRUE, legend.outside.position = &#39;right&#39;, legend.title.size =0.9, main.title = &#39;Per Capita Income, by Zipcode, Chicago 2018&#39;, main.title.size = 0.9) Use ColorBrewer To build aesthetically pleasing and easy-to-read maps, we recommend using color palette schemes recommended in ColorBrewer 2.0 developed by Cynthia Brewer. The website distinguishes between sequential(ordered), diverging(spread around a center) &amp; qualitative(categorical) data. Information on these palettes cab be displayed in R using RColorBrewer package. We can get the hex values for the colors used in a specific palette with n bins &amp; plot the corresponding colors using code below. require(RColorBrewer) RColorBrewer::brewer.pal(5,&quot;PuBuGn&quot;) ## [1] &quot;#F6EFF7&quot; &quot;#BDC9E1&quot; &quot;#67A9CF&quot; &quot;#1C9099&quot; &quot;#016C59&quot; RColorBrewer::display.brewer.pal(5,&quot;PuBuGn&quot;) We can update the jenks map by using this sequential color scheme and changing the transparency using alpha = 0.8 as below. tm_shape(chicagoZips) + tm_fill(&#39;perCapInc&#39;, title = &#39;Per Capita Income - Jenks&#39;, style = &#39;jenks&#39;, n = 5, palette = &#39;PuBuGn&#39;) + tm_borders() + tm_layout(frame = FALSE, legend.outside = TRUE, legend.outside.position = &#39;right&#39;, legend.title.size =0.9, main.title = &#39;Per Capita Income, by Zipcode, Chicago 2018&#39;, main.title.size = 0.9) We can also update the stdev map by using a diverging color scheme as below. tm_shape(chicagoZips) + tm_fill(&#39;perCapInc&#39;, title = &#39;Per Capita Income - Stdev&#39;, style = &#39;sd&#39;, palette = &#39;-RdBu&#39;, alpha = 0.9) + tm_borders() + tm_layout(frame = FALSE, legend.outside = TRUE, legend.outside.position = &#39;right&#39;, legend.title.size =0.9, main.title = &#39;Per Capita Income, by Zipcode, Chicago 2018&#39;, main.title.size = 0.9) "],["centroid-access-tutorial.html", "7 Nearest Resource Analysis 7.1 Overview 7.2 Environment Setup 7.3 Load data 7.4 Calculate centroids 7.5 Standardize CRS 7.6 Calculate Distance 7.7 Save Data", " 7 Nearest Resource Analysis 7.1 Overview Spatial Access to specific resource is often considered a multidimensional concept, though geographic distance is often central to the topic. Distance to the nearest resource is a common metric used to capture the availability of a resource, and in this tutorial we demonstrate how to calculate a minimum distance value from a ZCTA centroid to a set of resources, such as locations of methadone clinics. Each zip code will be assigned a “minimum distance access metric” as a value that indicates access to resources from that zip code. Our objectives are thus to: Generate centroids from areal data Calculate minimum distance from resources to area centroids Overlay resources and new minimum distance metric 7.2 Environment Setup To replicate the codes &amp; functions illustrated in this tutorial, you’ll need to have R and RStudio downloaded and installed on your system. This tutorial assumes some familiarity with the R programming language. 7.2.1 Packages used We will use the following packages in this tutorial: sf: to manipulate spatial data tmap: to visualize and create maps units: to convert units within spatial data 7.2.2 Input/Output Our inputs will be the clinic points we generated in previous tutorials, and the ZCTA boundary. Chicago Methadone Clinics, methadoneClinics.shp Chicago Zip Codes, chicago_zips.shp We will calculate the minimum distance between the resources and the centroids of the zip codes, then save the results as a shapefile and as a CSV. Our final result will be a shapefile/CSV with the minimum distance value for each zip. If you don’t have a shapefile of your data, but already have geographic coordinates as two columns in your CSV file, you can still use this tutorial. A reminder of how to transform your CSV with coordinates into a spatial data frame in R can be found here. 7.2.3 Load the packages Load the libraries for use. library(sf) library(tmap) library(units) 7.3 Load data First, load in the MOUD resources shapefile. Let’s take a look at the first few rows of the dataset. meth_sf &lt;- st_read(&quot;methadoneClinics.shp&quot;) ## Reading layer `methadoneClinics&#39; from data source `/Users/yashbansal/Desktop/CSDS_RA/opioid-environment-toolkit/methadoneClinics.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 27 features and 8 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -87.7349 ymin: 41.68698 xmax: -87.57673 ymax: 41.96475 ## geographic CRS: WGS 84 head(meth_sf) ## Simple feature collection with 6 features and 8 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -87.67818 ymin: 41.89268 xmax: -87.63409 ymax: 41.96475 ## geographic CRS: WGS 84 ## X Name ## 1 1 Chicago Treatment and Counseling Center, Inc. ## 2 2 Sundace Methadone Treatment Center, LLC ## 3 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview ## 4 4 PDSSC - Chicago, Inc. ## 5 5 Center for Addictive Problems, Inc. ## 6 6 Family Guidance Centers, Inc. ## Address City State Zip ## 1 4453 North Broadway st. Chicago IL 60640 ## 2 4545 North Broadway St. Chicago IL 60640 ## 3 3934 N. Lincoln Ave. Chicago IL 60613 ## 4 2260 N. Elston Ave. Chicago IL 60614 ## 5 609 N. Wells St. Chicago IL 60654 ## 6 310 W. Chicago Ave. Chicago IL 60654 ## fullAdd geo_method ## 1 4453 North Broadway st. Chicago IL 60640 osm ## 2 4545 North Broadway St. Chicago IL 60640 osm ## 3 3934 N. Lincoln Ave. Chicago IL 60613 census ## 4 2260 N. Elston Ave. Chicago IL 60614 census ## 5 609 N. Wells St. Chicago IL 60654 census ## 6 310 W. Chicago Ave. Chicago IL 60654 census ## geometry ## 1 POINT (-87.65566 41.96321) ## 2 POINT (-87.65694 41.96475) ## 3 POINT (-87.67818 41.95331) ## 4 POINT (-87.67407 41.92269) ## 5 POINT (-87.63409 41.89268) ## 6 POINT (-87.63636 41.89657) Next, we’ll load Chicago zip code boundaries. chicago_zips &lt;- read_sf(&quot;data/chicago_zips.shp&quot;) We can quickly plot our data for to confirm they loaded correctly, here using an interactive map: tm_shape(chicago_zips) + tm_borders() + tm_shape(meth_sf) + tm_dots(col = &quot;blue&quot;, size = 0.2) 7.4 Calculate centroids Now, we will calculate the centroids of the zip code boundaries. We will first need to project our data, which means change it from latitude and longitude to meaningful units, like ft or meters, so we can calculate distance properly. We’ll use the Illinois State Plane projection, with an EPSG code of 3435. Aside: To find the most appropriate projection for your data, do a Google Search for which projection works well - for state level data, each state has a State Plane projection with a specific code, known as the EPSG. I use epsg.io to check projections - here’s the New York State Plane page. Use the st_transform function to change the projection of the data. Notice how the values in geometry go from being relatively small (unprojected, lat/long) to very large (projected, in US feet). chicago_zips &lt;- st_transform(chicago_zips, 3435) chicago_zips ## Simple feature collection with 85 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 1058388 ymin: 1791133 xmax: 1205317 ymax: 1966816 ## projected CRS: NAD83 / Illinois East (ftUS) ## # A tibble: 85 x 10 ## ZCTA5CE10 GEOID10 CLASSFP10 MTFCC10 FUNCSTAT10 ALAND10 AWATER10 ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 60501 60501 B5 G6350 S 125322… 974360 ## 2 60007 60007 B5 G6350 S 364933… 917560 ## 3 60651 60651 B5 G6350 S 9052862 0 ## 4 60652 60652 B5 G6350 S 129878… 0 ## 5 60653 60653 B5 G6350 S 6041418 1696670 ## 6 60654 60654 B5 G6350 S 1464813 113471 ## 7 60655 60655 B5 G6350 S 114080… 0 ## 8 60656 60656 B5 G6350 S 8465226 0 ## 9 60657 60657 B5 G6350 S 5888324 2025836 ## 10 60659 60659 B5 G6350 S 5251086 2818 ## # … with 75 more rows, and 3 more variables: INTPTLAT10 &lt;chr&gt;, ## # INTPTLON10 &lt;chr&gt;, geometry &lt;MULTIPOLYGON [US_survey_foot]&gt; Then, we will calculate the centroids: chicago_centroids &lt;- st_centroid(chicago_zips) ## Warning in st_centroid.sf(chicago_zips): st_centroid assumes ## attributes are constant over geometries of x chicago_centroids ## Simple feature collection with 85 features and 9 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 1076716 ymin: 1802621 xmax: 1198093 ymax: 1956017 ## projected CRS: NAD83 / Illinois East (ftUS) ## # A tibble: 85 x 10 ## ZCTA5CE10 GEOID10 CLASSFP10 MTFCC10 FUNCSTAT10 ALAND10 AWATER10 ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 60501 60501 B5 G6350 S 125322… 974360 ## 2 60007 60007 B5 G6350 S 364933… 917560 ## 3 60651 60651 B5 G6350 S 9052862 0 ## 4 60652 60652 B5 G6350 S 129878… 0 ## 5 60653 60653 B5 G6350 S 6041418 1696670 ## 6 60654 60654 B5 G6350 S 1464813 113471 ## 7 60655 60655 B5 G6350 S 114080… 0 ## 8 60656 60656 B5 G6350 S 8465226 0 ## 9 60657 60657 B5 G6350 S 5888324 2025836 ## 10 60659 60659 B5 G6350 S 5251086 2818 ## # … with 75 more rows, and 3 more variables: INTPTLAT10 &lt;chr&gt;, ## # INTPTLON10 &lt;chr&gt;, geometry &lt;POINT [US_survey_foot]&gt; For each zip code, this will calculate the centroid, and the output will be a point dataset. Plot to double check that everything is ok. The st_geometry() function will once again just return the outline: plot(st_geometry(chicago_zips)) plot(st_geometry(chicago_centroids), add = TRUE, col = &quot;red&quot;) 7.4.1 Visualize &amp; Confirm Once again, we can create an interactive map: tm_shape(chicago_zips) + tm_borders() + tm_shape(chicago_centroids) + tm_dots() 7.5 Standardize CRS If we immediately try to calculate the distance between the zip centroids and the locations of the resources using the st_distance function, we’ll get an error: st_distance(chicago_centroids, meth_sf, by_element = TRUE) Error in st_distance(chicago_centroids, meth_sf, by_element = TRUE) : st_crs(x) == st_crs(y) is not TRUE Why is there an error? Because the projection of the centroids and the resource locations don’t match up. Let’s project the resource locations so that they match the projection of the centroids. First, use the st_crs function to check that the coordinate reference system (or projection) is the same. They’re not, so we have to fix it. st_crs(chicago_centroids) ## Coordinate Reference System: ## User input: EPSG:3435 ## wkt: ## PROJCRS[&quot;NAD83 / Illinois East (ftUS)&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;SPCS83 Illinois East zone (US Survey feet)&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,36.6666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-88.3333333333333, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.999975, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,984250, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;USA - Illinois - SPCS - E&quot;], ## BBOX[37.06,-89.28,42.5,-87.02]], ## ID[&quot;EPSG&quot;,3435]] st_crs(meth_sf) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] We’ll take the CRS from the zip code centroids data, and use it as input to st_transform applied to the methadone clinics data. new_crs &lt;- st_crs(chicago_centroids) new_crs ## Coordinate Reference System: ## User input: EPSG:3435 ## wkt: ## PROJCRS[&quot;NAD83 / Illinois East (ftUS)&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;SPCS83 Illinois East zone (US Survey feet)&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,36.6666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-88.3333333333333, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.999975, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,984250, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;USA - Illinois - SPCS - E&quot;], ## BBOX[37.06,-89.28,42.5,-87.02]], ## ID[&quot;EPSG&quot;,3435]] meth_sf &lt;- st_transform(meth_sf, new_crs) If we check the CRS again, we now see that they match. Mismatched projections are a commonly made mistake in geospatial data processing. st_crs(chicago_centroids) ## Coordinate Reference System: ## User input: EPSG:3435 ## wkt: ## PROJCRS[&quot;NAD83 / Illinois East (ftUS)&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;SPCS83 Illinois East zone (US Survey feet)&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,36.6666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-88.3333333333333, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.999975, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,984250, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;USA - Illinois - SPCS - E&quot;], ## BBOX[37.06,-89.28,42.5,-87.02]], ## ID[&quot;EPSG&quot;,3435]] st_crs(meth_sf) ## Coordinate Reference System: ## User input: EPSG:3435 ## wkt: ## PROJCRS[&quot;NAD83 / Illinois East (ftUS)&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;SPCS83 Illinois East zone (US Survey feet)&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,36.6666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-88.3333333333333, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.999975, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,984250, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;USA - Illinois - SPCS - E&quot;], ## BBOX[37.06,-89.28,42.5,-87.02]], ## ID[&quot;EPSG&quot;,3435]] Now we have the zip boundaries, the centroids of the zips, and the resource locations, as shown below. Next, we will calculate the distance to the nearest resource from each zip code centroid. plot(st_geometry(chicago_zips)) plot(st_geometry(chicago_centroids), col = &quot;red&quot;, add = TRUE) plot(st_geometry(meth_sf), col = &quot;blue&quot;, add = TRUE) 7.6 Calculate Distance First, we’ll identify the resource that is the closest to a zip centroid using the st_nearest_feature function. (It will return the index of the object that is nearest, so we will subset the resources by the index to get the nearest object.) nearest_clinic_indexes &lt;- st_nearest_feature(chicago_centroids, meth_sf) nearest_clinic &lt;- meth_sf[nearest_clinic_indexes,] nearest_clinic ## Simple feature collection with 85 features and 8 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 1147259 ymin: 1829330 xmax: 1190680 ymax: 1930471 ## projected CRS: NAD83 / Illinois East (ftUS) ## First 10 features: ## X ## 16 16 ## 7 7 ## 7.1 7 ## 26 26 ## 15 15 ## 5 5 ## 26.1 26 ## 7.2 7 ## 1 1 ## 3 3 ## Name ## 16 Katherine Boone Robinson Foundation ## 7 A Rincon Family Services ## 7.1 A Rincon Family Services ## 26 New Hope Community Service Center ## 15 HRDI- Grand Boulevard Professional Counseling Center ## 5 Center for Addictive Problems, Inc. ## 26.1 New Hope Community Service Center ## 7.2 A Rincon Family Services ## 1 Chicago Treatment and Counseling Center, Inc. ## 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview ## Address City State Zip ## 16 4100 W. Ogden Ave. Chicago IL 60623 ## 7 3809 W. Grand Ave. Chicago IL 60651 ## 7.1 3809 W. Grand Ave. Chicago IL 60651 ## 26 2559 W. 79th St. Chicago IL 60652 ## 15 340 E. 51st St. Chicago IL 60615 ## 5 609 N. Wells St. Chicago IL 60654 ## 26.1 2559 W. 79th St. Chicago IL 60652 ## 7.2 3809 W. Grand Ave. Chicago IL 60651 ## 1 4453 North Broadway st. Chicago IL 60640 ## 3 3934 N. Lincoln Ave. Chicago IL 60613 ## fullAdd geo_method ## 16 4100 W. Ogden Ave. Chicago IL 60623 census ## 7 3809 W. Grand Ave. Chicago IL 60651 census ## 7.1 3809 W. Grand Ave. Chicago IL 60651 census ## 26 2559 W. 79th St. Chicago IL 60652 osm ## 15 340 E. 51st St. Chicago IL 60615 census ## 5 609 N. Wells St. Chicago IL 60654 census ## 26.1 2559 W. 79th St. Chicago IL 60652 osm ## 7.2 3809 W. Grand Ave. Chicago IL 60651 census ## 1 4453 North Broadway st. Chicago IL 60640 osm ## 3 3934 N. Lincoln Ave. Chicago IL 60613 census ## geometry ## 16 POINT (1149437 1888620) ## 7 POINT (1150707 1908328) ## 7.1 POINT (1150707 1908328) ## 26 POINT (1160422 1852071) ## 15 POINT (1179319 1871281) ## 5 POINT (1174632 1904257) ## 26.1 POINT (1160422 1852071) ## 7.2 POINT (1150707 1908328) ## 1 POINT (1168556 1929911) ## 3 POINT (1162460 1926257) Then, we will calculate the distance between the nearest resource and the zip code centroid with the st_distance function. As shown above, make sure both of your datasets are projected, and in the same projection, before you run st_distance. min_dists &lt;- st_distance(chicago_centroids, nearest_clinic, by_element = TRUE) min_dists ## Units: [US_survey_foot] ## [1] 36764.471 82821.757 5237.764 7419.050 7241.215 873.991 ## [7] 20515.835 38337.779 8516.342 15479.988 9823.172 4447.705 ## [13] 33707.202 24111.173 24184.991 45224.230 31198.648 10113.372 ## [19] 10890.143 13779.075 49874.400 32463.844 36633.404 21981.243 ## [25] 13637.327 22151.171 63268.911 4242.008 3749.998 5116.415 ## [31] 5530.724 8696.602 3965.649 4368.285 6998.725 6937.025 ## [37] 3967.509 45944.187 32598.459 44548.952 58483.551 5416.487 ## [43] 5728.851 2327.899 6646.411 5817.262 365.628 13619.270 ## [49] 7021.182 5024.536 2829.488 1648.361 7709.690 2628.400 ## [55] 1521.495 9423.848 16648.775 2185.237 11463.785 22537.644 ## [61] 11872.133 7759.662 39665.409 11741.279 18689.070 27555.396 ## [67] 5301.820 8808.542 25363.914 18986.590 26456.564 7562.917 ## [73] 3496.941 11023.379 3128.000 16826.923 6172.874 25276.556 ## [79] 17290.797 15198.524 25164.072 18438.696 19972.965 33211.229 ## [85] 22518.257 This is in US feet. To change to a more meaningful unit, such as miles, we can use the set_units() function: min_dists_mi &lt;- set_units(min_dists, &quot;mi&quot;) min_dists_mi ## Units: [mi] ## [1] 6.96298198 15.68597011 0.99200275 1.40512596 1.37144503 ## [6] 0.16552893 3.88558262 7.26095754 1.61294677 2.93182182 ## [11] 1.86045293 0.84236999 6.38395252 4.56651915 4.58049994 ## [16] 8.56521226 5.90884663 1.91541520 2.06253115 2.60967860 ## [21] 9.44592795 6.14846754 6.93815858 4.16312245 2.58283224 ## [26] 4.19530595 11.98277234 0.80341215 0.71022833 0.96901998 ## [31] 1.04748778 1.64708702 0.75107138 0.82732834 1.32551867 ## [36] 1.31383303 0.75142366 8.70156788 6.17396283 8.43731838 ## [41] 11.07645233 1.02585189 1.08501190 0.44089088 1.25879245 ## [46] 1.10175645 0.06924787 2.57941243 1.32977202 0.95161851 ## [51] 0.53588899 0.31219014 1.46017146 0.49780409 0.28816254 ## [56] 1.78482335 3.15318335 0.41387148 2.17117583 4.26850167 ## [61] 2.24851459 1.46963585 7.51240307 2.22373150 3.53960367 ## [66] 5.21883539 1.00413457 1.66828778 4.80378114 3.59595236 ## [71] 5.01072284 1.43237347 0.66230081 2.08776534 0.59242542 ## [76] 3.18692368 1.16910723 4.78723618 3.27477863 2.87851400 ## [81] 4.76593224 3.49218425 3.78276617 6.29001804 4.26482998 7.6.1 Merge Data We then rejoin the minimum distances to the zip code data, by column binding min_dists_mi to the original chicago_zips data. min_dist_sf &lt;- cbind(chicago_zips, min_dists_mi) min_dist_sf ## Simple feature collection with 85 features and 10 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 1058388 ymin: 1791133 xmax: 1205317 ymax: 1966816 ## projected CRS: NAD83 / Illinois East (ftUS) ## First 10 features: ## ZCTA5CE10 GEOID10 CLASSFP10 MTFCC10 FUNCSTAT10 ALAND10 ## 1 60501 60501 B5 G6350 S 12532295 ## 2 60007 60007 B5 G6350 S 36493383 ## 3 60651 60651 B5 G6350 S 9052862 ## 4 60652 60652 B5 G6350 S 12987857 ## 5 60653 60653 B5 G6350 S 6041418 ## 6 60654 60654 B5 G6350 S 1464813 ## 7 60655 60655 B5 G6350 S 11408010 ## 8 60656 60656 B5 G6350 S 8465226 ## 9 60657 60657 B5 G6350 S 5888324 ## 10 60659 60659 B5 G6350 S 5251086 ## AWATER10 INTPTLAT10 INTPTLON10 min_dists_mi ## 1 974360 +41.7802209 -087.8232440 6.9629820 [mi] ## 2 917560 +42.0086000 -087.9973398 15.6859701 [mi] ## 3 0 +41.9020934 -087.7408565 0.9920027 [mi] ## 4 0 +41.7479319 -087.7147951 1.4051260 [mi] ## 5 1696670 +41.8199645 -087.6059654 1.3714450 [mi] ## 6 113471 +41.8918225 -087.6383036 0.1655289 [mi] ## 7 0 +41.6947762 -087.7037764 3.8855826 [mi] ## 8 0 +41.9742800 -087.8271283 7.2609575 [mi] ## 9 2025836 +41.9402931 -087.6468569 1.6129468 [mi] ## 10 2818 +41.9914885 -087.7039859 2.9318218 [mi] ## geometry ## 1 MULTIPOLYGON (((1112613 185... ## 2 MULTIPOLYGON (((1058389 194... ## 3 MULTIPOLYGON (((1136069 190... ## 4 MULTIPOLYGON (((1145542 185... ## 5 MULTIPOLYGON (((1177007 187... ## 6 MULTIPOLYGON (((1170904 190... ## 7 MULTIPOLYGON (((1146378 183... ## 8 MULTIPOLYGON (((1110359 193... ## 9 MULTIPOLYGON (((1162394 192... ## 10 MULTIPOLYGON (((1148555 194... 7.6.2 Visualize &amp; Confirm We can now visualize the zip-level access to methadone clinics using our new access metric, using the tmap package. We’ll use quantile bins with five intervals. tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tm_shape(min_dist_sf) + tm_polygons(&quot;min_dists_mi&quot;, style = &#39;quantile&#39;, n=5, title = &quot;Minimum Distance (mi)&quot;) + tm_layout(main.title = &quot;Minimum Distance from Zip Centroid\\n to Methadone Clinic&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Access by zip code can also be combined with locations of resources: tm_shape(min_dist_sf) + tm_polygons(&quot;min_dists_mi&quot;, style = &#39;quantile&#39;, n=5, title = &quot;Minimum Distance (mi)&quot;) + tm_shape(meth_sf) + tm_dots(size = 0.2) + tm_layout(main.title = &quot;Minimum Distance from Zip Centroid\\n to Methadone Clinic&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) head(chicago_zips) ## Simple feature collection with 6 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 1058388 ymin: 1846408 xmax: 1189038 ymax: 1957548 ## projected CRS: NAD83 / Illinois East (ftUS) ## # A tibble: 6 x 10 ## ZCTA5CE10 GEOID10 CLASSFP10 MTFCC10 FUNCSTAT10 ALAND10 AWATER10 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 60501 60501 B5 G6350 S 125322… 974360 ## 2 60007 60007 B5 G6350 S 364933… 917560 ## 3 60651 60651 B5 G6350 S 9052862 0 ## 4 60652 60652 B5 G6350 S 129878… 0 ## 5 60653 60653 B5 G6350 S 6041418 1696670 ## 6 60654 60654 B5 G6350 S 1464813 113471 ## # … with 3 more variables: INTPTLAT10 &lt;chr&gt;, INTPTLON10 &lt;chr&gt;, ## # geometry &lt;MULTIPOLYGON [US_survey_foot]&gt; 7.7 Save Data To save our final result to a CSV: write_sf(min_dist_sf, &quot;chizips_Access.csv&quot;) We can also write out this data to a shapefile format: write_sf(min_dist_sf, &quot;chizips_Access.shp&quot;) "]]
